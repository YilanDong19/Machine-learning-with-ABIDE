{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tCrjPOHeJtJM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670447743643,"user_tz":0,"elapsed":26188,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"50425249-ca38-42fb-bf27-104d20366d3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/MyDrive/ABIDE/\") "]},{"cell_type":"code","source":["!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n","!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","!pip install torch-geometric\n","import torch\n","print(torch.__version__)  \n","print(torch.version.cuda)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Tu43fpGJc-O0","executionInfo":{"status":"ok","timestamp":1670447892507,"user_tz":0,"elapsed":148870,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"d111451d-43a8-451f-d08a-799cc4605e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.11.0\n","  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n","\u001b[K     |████████████████████████████████| 750.6 MB 15 kB/s \n","\u001b[?25hCollecting torchvision==0.12.0\n","  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[K     |████████████████████████████████| 21.0 MB 1.4 MB/s \n","\u001b[?25hCollecting torchaudio==0.11.0\n","  Downloading torchaudio-0.11.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0) (4.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.0+cu116\n","    Uninstalling torchvision-0.14.0+cu116:\n","      Successfully uninstalled torchvision-0.14.0+cu116\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.13.0+cu116\n","    Uninstalling torchaudio-0.13.0+cu116:\n","      Successfully uninstalled torchaudio-0.13.0+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu102/torch_scatter-2.0.9-cp38-cp38-linux_x86_64.whl (8.0 MB)\n","\u001b[K     |████████████████████████████████| 8.0 MB 6.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu102/torch_sparse-0.6.15-cp38-cp38-linux_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n","Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.15\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu102/torch_cluster-1.6.0-cp38-cp38-linux_x86_64.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 7.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.0\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0%2Bcu102.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu102/torch_spline_conv-1.2.1-cp38-cp38-linux_x86_64.whl (672 kB)\n","\u001b[K     |████████████████████████████████| 672 kB 576 kB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n","\u001b[K     |████████████████████████████████| 564 kB 20.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n","Collecting psutil>=5.8.0\n","  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n","\u001b[K     |████████████████████████████████| 280 kB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.9.24)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=8f5dd73766daf728c88da63ae7dbf5d6f9842c47698a89a2bc084affd3c9722b\n","  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n","Successfully built torch-geometric\n","Installing collected packages: psutil, torch-geometric\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["psutil"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1.11.0+cu102\n","10.2\n"]}]},{"cell_type":"code","source":["# load ComBat algorithm\n","# !python /content/drive/MyDrive/ABIDE/neuralCombat.py \n","import sys\n","sys.path.append('/content/drive/MyDrive/ABIDE/')"],"metadata":{"id":"h35b8s1aYHzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWp3uGG8S9wK"},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from neuralCombat import *\n","# from neuralCombat.py import neuralCombat\n","import pandas as pd\n","from sklearn import svm\n","import joblib\n","import openpyxl\n","from openpyxl import load_workbook\n","import os\n","import scipy.io as scio\n","import argparse\n","import numpy as np\n","import time\n","import torch\n","import torch.utils.data\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data.dataset import Dataset\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import make_grid , save_image\n","import torchvision.utils as vutils\n","from os.path import join\n","from os import listdir\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import DataLoader\n","from collections import OrderedDict\n","import nibabel as nib\n","import matplotlib.pyplot as plt\n","import cv2 as cv\n","from os import path\n","import shutil\n","import scipy.stats\n","import scipy.ndimage\n","import random\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","import sys\n","import math\n","from functools import reduce\n","import operator\n","from scipy.interpolate import interp1d\n","from torch.optim import lr_scheduler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.feature_selection import RFE\n","\n","def feature_selection(matrix, labels, train_ind, fnum):\n","    \"\"\"\n","        matrix       : feature matrix (num_subjects x num_features)\n","        labels       : ground truth labels (num_subjects x 1)\n","        train_ind    : indices of the training samples\n","        fnum         : size of the feature vector after feature selection \n","\n","    return:\n","        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n","    \"\"\"\n","\n","    estimator = RidgeClassifier()\n","    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n","\n","    featureX = matrix[train_ind, :]\n","    featureY = labels[train_ind]\n","    selector = selector.fit(featureX, featureY.ravel())\n","\n","    return selector\n","\n","def get_index(lst=None, item=''):\n","\treturn [i for i in range(len(lst)) if lst[i] == item]\n","\n","def flatten_one(length, img):\n","\t'''\n","\t  In some situations, the dimension on z-axis of images are smaller than\n","\t  the dimension of z-axis of patches, this function will be used to pad\n","\t'''\n","\n","\tone_line = np.zeros((1, int(length)))\n","\tposition = 0\n","\tfor i in range(img.shape[0]):  # column\n","\t\tfor j in range(i + 1, img.shape[1]):  # row\n","\t\t\tone_line[0, position] = img[j, i]\n","\t\t\tposition = position + 1\n","\treturn one_line\n","\n","\n","def get_ids(num_subjects=None,dir_path=''):\n","\t\"\"\"\n","\treturn:\n","\t\tsubject_IDs    : list of all subject IDs\n","\t\"\"\"\n","\tsubject_IDs = np.genfromtxt(os.path.join(dir_path, 'subject_IDs.txt'), dtype=str)\n","\n","\tif num_subjects is not None:\n","\t\tsubject_IDs = subject_IDs[:num_subjects]\n","\n","\treturn subject_IDs\n","\n","\n","def save_model(net,path, name_net):\n","\n","  # This fucntion is used to save a specific model\n","\n","    path_net =  path + '/' + name_net + '.pth'\n","    torch.save(net.cpu().state_dict(), path_net)\n","    net.cuda()\n","\n","def load_model(net, path, name_net):\n","\n","  # This function is used to load a specific model we saved before\n","\n","    path_net =  path + '/' + name_net + '.pth'\n","    net.load_state_dict(torch.load(path_net))\n","\n","    return net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2V5v9WfVuZji","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670447927108,"user_tz":0,"elapsed":31295,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"98f1ed31-f84e-4c46-9e55-cae66e93f2f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-f202fb8b3ef0>:60: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  EV_GCN_site = np.zeros([number_samples - len(useless_samples)], dtype=np.int)\n","<ipython-input-5-f202fb8b3ef0>:62: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  EV_GCN_gender = np.zeros([number_samples - len(useless_samples)], dtype=np.int)\n"]},{"output_type":"stream","name":"stdout","text":["constructing fMRI features\n","50003\n","50004\n","50005\n","50006\n","50007\n","50008\n","50010\n","50011\n","50012\n","50013\n","50014\n","50015\n","50016\n","50020\n","50022\n","50023\n","50024\n","50025\n","50026\n","50027\n","50028\n","50030\n","50031\n","50032\n","50033\n","50034\n","50035\n","50036\n","50037\n","50038\n","50039\n","50040\n","50041\n","50042\n","50043\n","50044\n","50045\n","50046\n","50047\n","50048\n","50049\n","50050\n","50051\n","50052\n","50053\n","50054\n","50056\n","50057\n","50059\n","50060\n","50102\n","50103\n","50104\n","50105\n","50106\n","50107\n","50109\n","50111\n","50112\n","50113\n","50114\n","50115\n","50116\n","50117\n","50118\n","50119\n","50121\n","50123\n","50124\n","50125\n","50127\n","50128\n","50129\n","50130\n","50131\n","50132\n","50134\n","50135\n","50142\n","50143\n","50144\n","50145\n","50146\n","50147\n","50148\n","50149\n","50150\n","50152\n","50153\n","50156\n","50157\n","50158\n","50159\n","50160\n","50161\n","50162\n","50163\n","50164\n","50167\n","50168\n","50169\n","50170\n","50171\n","50182\n","50183\n","50184\n","50186\n","50187\n","50188\n","50189\n","50190\n","50193\n","50194\n","50195\n","50196\n","50198\n","50199\n","50200\n","50201\n","50202\n","50203\n","50204\n","50205\n","50206\n","50208\n","50210\n","50213\n","50214\n","50215\n","50217\n","50232\n","50233\n","50234\n","50236\n","50237\n","50239\n","50240\n","50241\n","50243\n","50245\n","50247\n","50248\n","50249\n","50250\n","50251\n","50252\n","50253\n","50254\n","50255\n","50257\n","50259\n","50260\n","50261\n","50262\n","50263\n","50264\n","50265\n","50266\n","50267\n","50268\n","50269\n","50270\n","50271\n","50272\n","50273\n","50274\n","50275\n","50276\n","50278\n","50282\n","50284\n","50285\n","50287\n","50289\n","50290\n","50291\n","50292\n","50293\n","50294\n","50295\n","50297\n","50298\n","50300\n","50301\n","50302\n","50304\n","50308\n","50310\n","50312\n","50314\n","50315\n","50318\n","50319\n","50320\n","50321\n","50324\n","50325\n","50327\n","50329\n","50330\n","50331\n","50332\n","50333\n","50334\n","50335\n","50336\n","50337\n","50338\n","50339\n","50340\n","50341\n","50342\n","50343\n","50344\n","50345\n","50346\n","50347\n","50348\n","50349\n","50350\n","50351\n","50352\n","50353\n","50354\n","50355\n","50356\n","50357\n","50358\n","50359\n","50360\n","50361\n","50362\n","50363\n","50364\n","50365\n","50366\n","50367\n","50368\n","50369\n","50370\n","50372\n","50373\n","50374\n","50375\n","50376\n","50377\n","50379\n","50380\n","50381\n","50382\n","50383\n","50385\n","50386\n","50387\n","50388\n","50390\n","50391\n","50397\n","50399\n","50402\n","50403\n","50404\n","50405\n","50406\n","50407\n","50408\n","50410\n","50411\n","50412\n","50413\n","50414\n","50415\n","50416\n","50417\n","50418\n","50419\n","50421\n","50422\n","50424\n","50425\n","50426\n","50427\n","50428\n","50433\n","50434\n","50435\n","50436\n","50437\n","50438\n","50439\n","50440\n","50441\n","50442\n","50443\n","50444\n","50445\n","50446\n","50447\n","50448\n","50449\n","50453\n","50463\n","50466\n","50467\n","50468\n","50469\n","50470\n","50477\n","50480\n","50481\n","50482\n","50483\n","50485\n","50486\n","50487\n","50488\n","50490\n","50491\n","50492\n","50493\n","50494\n","50496\n","50497\n","50498\n","50499\n","50500\n","50501\n","50502\n","50503\n","50504\n","50507\n","50509\n","50510\n","50514\n","50515\n","50516\n","50518\n","50519\n","50520\n","50521\n","50523\n","50524\n","50525\n","50526\n","50527\n","50528\n","50529\n","50530\n","50531\n","50532\n","50551\n","50552\n","50555\n","50557\n","50558\n","50561\n","50563\n","50565\n","50568\n","50569\n","50570\n","50571\n","50572\n","50573\n","50574\n","50575\n","50576\n","50577\n","50578\n","50601\n","50602\n","50603\n","50604\n","50606\n","50607\n","50608\n","50612\n","50613\n","50614\n","50615\n","50616\n","50619\n","50620\n","50621\n","50622\n","50623\n","50624\n","50625\n","50626\n","50627\n","50628\n","50642\n","50644\n","50647\n","50648\n","50649\n","50654\n","50656\n","50659\n","50664\n","50665\n","50669\n","50682\n","50683\n","50685\n","50686\n","50687\n","50688\n","50689\n","50690\n","50691\n","50692\n","50693\n","50694\n","50695\n","50696\n","50697\n","50698\n","50699\n","50700\n","50701\n","50702\n","50703\n","50704\n","50705\n","50706\n","50707\n","50708\n","50709\n","50711\n","50722\n","50723\n","50724\n","50725\n","50726\n","50728\n","50730\n","50731\n","50733\n","50735\n","50737\n","50738\n","50739\n","50740\n","50741\n","50742\n","50743\n","50744\n","50745\n","50748\n","50749\n","50750\n","50751\n","50752\n","50754\n","50755\n","50756\n","50757\n","50772\n","50773\n","50774\n","50775\n","50776\n","50777\n","50778\n","50780\n","50781\n","50782\n","50783\n","50786\n","50790\n","50791\n","50792\n","50796\n","50797\n","50798\n","50799\n","50800\n","50801\n","50803\n","50807\n","50812\n","50814\n","50816\n","50817\n","50818\n","50820\n","50821\n","50822\n","50823\n","50824\n","50952\n","50954\n","50955\n","50956\n","50957\n","50958\n","50959\n","50960\n","50961\n","50962\n","50964\n","50965\n","50966\n","50967\n","50968\n","50969\n","50970\n","50972\n","50973\n","50974\n","50976\n","50977\n","50978\n","50979\n","50981\n","50982\n","50983\n","50984\n","50985\n","50986\n","50987\n","50988\n","50989\n","50990\n","50991\n","50992\n","50993\n","50994\n","50995\n","50996\n","50997\n","50999\n","51000\n","51001\n","51002\n","51003\n","51006\n","51007\n","51008\n","51009\n","51010\n","51011\n","51012\n","51013\n","51014\n","51015\n","51016\n","51017\n","51018\n","51019\n","51020\n","51021\n","51023\n","51024\n","51025\n","51026\n","51027\n","51028\n","51029\n","51030\n","51032\n","51033\n","51034\n","51035\n","51036\n","51038\n","51039\n","51040\n","51041\n","51042\n","51044\n","51045\n","51046\n","51047\n","51048\n","51049\n","51050\n","51051\n","51052\n","51053\n","51054\n","51055\n","51056\n","51057\n","51058\n","51059\n","51060\n","51061\n","51062\n","51063\n","51064\n","51065\n","51066\n","51067\n","51068\n","51069\n","51070\n","51072\n","51073\n","51074\n","51075\n","51076\n","51077\n","51078\n","51079\n","51080\n","51081\n","51082\n","51083\n","51084\n","51085\n","51086\n","51087\n","51088\n","51089\n","51090\n","51091\n","51093\n","51094\n","51095\n","51096\n","51097\n","51098\n","51099\n","51100\n","51101\n","51102\n","51103\n","51104\n","51105\n","51106\n","51107\n","51109\n","51110\n","51111\n","51112\n","51113\n","51114\n","51116\n","51117\n","51118\n","51122\n","51123\n","51124\n","51126\n","51127\n","51128\n","51129\n","51130\n","51131\n","51132\n","51133\n","51134\n","51135\n","51136\n","51137\n","51138\n","51139\n","51140\n","51141\n","51142\n","51146\n","51147\n","51148\n","51149\n","51150\n","51151\n","51152\n","51153\n","51154\n","51155\n","51156\n","51159\n","51161\n","51162\n","51163\n","51164\n","51168\n","51169\n","51170\n","51171\n","51173\n","51177\n","51178\n","51179\n","51180\n","51181\n","51182\n","51183\n","51184\n","51185\n","51187\n","51188\n","51189\n","51192\n","51194\n","51197\n","51198\n","51201\n","51202\n","51203\n","51204\n","51205\n","51206\n","51207\n","51208\n","51210\n","51211\n","51212\n","51214\n","51215\n","51216\n","51217\n","51218\n","51219\n","51220\n","51221\n","51222\n","51223\n","51224\n","51225\n","51226\n","51228\n","51229\n","51230\n","51231\n","51234\n","51235\n","51236\n","51237\n","51239\n","51240\n","51241\n","51248\n","51249\n","51250\n","51251\n","51252\n","51253\n","51254\n","51255\n","51256\n","51257\n","51260\n","51261\n","51262\n","51264\n","51265\n","51266\n","51267\n","51268\n","51269\n","51271\n","51272\n","51273\n","51275\n","51276\n","51277\n","51278\n","51279\n","51280\n","51281\n","51291\n","51292\n","51293\n","51294\n","51295\n","51297\n","51298\n","51299\n","51300\n","51301\n","51302\n","51303\n","51304\n","51305\n","51306\n","51307\n","51308\n","51309\n","51311\n","51313\n","51315\n","51318\n","51319\n","51320\n","51321\n","51322\n","51323\n","51325\n","51326\n","51327\n","51328\n","51329\n","51330\n","51331\n","51332\n","51333\n","51334\n","The index for sample 51334 is :  [799]\n","51335\n","51336\n","51338\n","51339\n","51340\n","51341\n","51342\n","51343\n","51344\n","51345\n","51346\n","51347\n","51349\n","51350\n","51351\n","51354\n","51356\n","51357\n","51359\n","51360\n","51361\n","51362\n","51363\n","51364\n","51365\n","51369\n","51370\n","51373\n","51461\n","51463\n","51464\n","51465\n","51473\n","51477\n","51479\n","51480\n","51481\n","51482\n","51484\n","51487\n","51488\n","51491\n","51493\n","51556\n","51557\n","51558\n","51559\n","51560\n","51562\n","51563\n","51564\n","51565\n","51566\n","51567\n","51568\n","51569\n","51570\n","51572\n","51573\n","51574\n","51576\n","51577\n","51578\n","51579\n","51580\n","51582\n","51583\n","51584\n","51585\n","51606\n","51607\n","Done\n"]}],"source":["########################################### Load Data ###############################################\n","#####################################################################################################\n","#####################################################################################################\n","fMRI_atlas = 'CC200'  # AAL or CC200\n","combat = True    # True or False\n","scaler = True\n","\n","if combat == False:\n","  save_combat = '/without_ComBat/'\n","else:\n","  save_combat = '/with_ComBat/'\n","\n","save_path = '/content/drive/MyDrive/ABIDE/save_models/'+fMRI_atlas+ save_combat\n","root_path = '/content/dataset'\n","if os.path.exists(root_path):\n","  shutil.rmtree(root_path) \n","shutil.copytree('/content/drive/MyDrive/ABIDE/' + fMRI_atlas + '/original/', root_path) \n","label_dir = '/content/drive/MyDrive/ABIDE/phenotypic_image_quality'\n","\n","k_fold = 5\n","new_number_features = 5000\n","number_samples = 871\n","useless_samples = ['51334']\n","if fMRI_atlas == 'AAL':\n","  image_size = [116, 116]\n","else:\n","  image_size = [200, 200]\n","\n","\n","age_name = 'ages.mat'\n","gender_name = 'genders.mat'\n","label_name = 'ABIDE_label_871.mat'\n","label = scio.loadmat(os.path.join(label_dir, label_name))\n","label = label['label'][0]\n","labels = np.zeros(number_samples - len(useless_samples))\n","site_name = 'sites.mat'\n","site = scio.loadmat(os.path.join(label_dir, site_name))\n","all_sites = site['sites']\n","for i in range(len(all_sites)):\n","\tsite = all_sites[i]\n","\tall_sites[i] = site.replace(' ', '')\n","unique_sites = np.unique(all_sites)\n","sites = []\n","unique_sites = list(unique_sites)\n","\n","age = scio.loadmat(os.path.join(label_dir, age_name))\n","age = age['ages']\n","ages = np.zeros((number_samples - len(useless_samples), 1))\n","\n","gender = scio.loadmat(os.path.join(label_dir, gender_name))\n","gender = gender['genders']\n","genders = np.zeros((number_samples - len(useless_samples), 1))\n","\n","subject_IDs = get_ids(number_samples,dir_path=label_dir)\n","subject_IDs = subject_IDs.tolist()\n","length = image_size[0] * (image_size[1]-1) / 2\n","fMRI_images = np.zeros((number_samples - len(useless_samples), int(length)))\n","\n","EV_GCN_unique = list(unique_sites)\n","EV_GCN_site = np.zeros([number_samples - len(useless_samples)], dtype=np.int)\n","EV_GCN_age = np.zeros([number_samples - len(useless_samples)], dtype=np.float32)\n","EV_GCN_gender = np.zeros([number_samples - len(useless_samples)], dtype=np.int)\n","\n","position = 0\n","print('constructing fMRI features')\n","for i in range(number_samples):\n","  subject_name = subject_IDs[i]\n","  print(subject_name)\n","  if subject_name in useless_samples:\n","    subject_index = get_index(lst=subject_IDs, item=subject_name)\n","    print('The index for sample ' + subject_name + ' is : ', subject_index)\n","  else:\n","    image_name = subject_name + '.mat'\n","    subject_index = get_index(lst=subject_IDs, item=subject_name)\n","    image = scio.loadmat(os.path.join(root_path, image_name))\n","    img = image['connectivity']\n","\n","    idx = np.triu_indices_from(img, 1)\n","    fMRI_images[position, :] = img[idx]\n","    sites.append(all_sites[subject_index[0]])\n","    labels[position] = label[subject_index[0]]\n","    genders[position] = int(gender[subject_index[0]]) + 1\n","    ages[position] = float(age[subject_index[0]])\n","    \n","    position = position + 1\n","\n","for i in range(number_samples - len(useless_samples)): \n","\n","  EV_GCN_site[i] = EV_GCN_unique.index(sites[i])\n","  EV_GCN_age[i] = ages[i]\n","  EV_GCN_gender[i] = genders[i]\n","\n","phonetic_data = np.zeros([number_samples - len(useless_samples), 3], dtype=np.float32)\n","phonetic_data[:,0] = EV_GCN_site \n","phonetic_data[:,1] = EV_GCN_gender \n","phonetic_data[:,2] = EV_GCN_age\n","\n","pd_dict = {}\n","pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n","pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n","pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n","\n","if scaler == True:\n","\tfMRI_images = StandardScaler().fit_transform(fMRI_images)\n","\tprint('Done')\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARLzsYrrJEo0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670447953399,"user_tz":0,"elapsed":26306,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"c6c8a943-7378-4406-8d46-381c24c7e0cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[neuroCombat] Creating design matrix\n","[neuroCombat] Standardizing data across features\n","[neuroCombat] Fitting L/S model and finding priors\n","[neuroCombat] Finding parametric adjustments\n","[neuroCombat] Final adjustment of data\n"]}],"source":["############################################### Combat ###############################################################\n","############################################### Combat ###############################################################\n","############################################### Combat ###############################################################\n","if combat == True:\n","\tbatch = []\n","\tfor i in range(len(sites)):\n","\t\tbatch.append(get_index(lst=unique_sites, item=sites[i])[0]+1)\n","\n","\tcombat_labels = []\n","\tcombat_genders = []\n","\tcombat_ages = []\n","\tfor i in range(len(labels)):\n","\t\tcombat_labels.append(labels[i] + 1)\n","\t\tcombat_genders.append(genders[i])\n","\t\tcombat_ages.append(ages[i])\n","\n","\tnew_all_samples_feature = fMRI_images.T\n","\tcovars = {}\n","\tcovars['batch'] = batch\n","\tcovars['labels'] = combat_labels\n","\tcovars['genders'] = combat_genders\n","\tcovars['ages'] = combat_ages\n","\tcovars = pd.DataFrame(covars)\n","\n","\t# To specify names of the variables that are categorical:\n","\tcategorical_cols = ['labels', 'genders']\n","\t# To specify the name of the variable that encodes for the scanner/batch covariate:\n","\tbatch_col = 'batch'\n","\tcontinuous_cols = ['ages']\n","\t# Harmonization step:\n","\tnew_all_samples_feature = neuroCombat(dat=new_all_samples_feature,\n","\t\t\t\t\t\t\t\t\t\t  covars=covars,\n","\t\t\t\t\t\t\t\t\t\t  batch_col=batch_col,\n","\t\t\t\t\t\t\t\t\t\t  categorical_cols=categorical_cols,\n","\t\t\t\t\t\t\t\t\t\t\tcontinuous_cols=continuous_cols)[\"data\"]\n","\tfMRI_images = new_all_samples_feature.T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU8Ym4vRuOkJ"},"outputs":[],"source":["dist_train = {}\n","dist_validation = {}\n","dist_test = {}\n","for i in range(k_fold):\n","\tdist_train[str(i + 1)] = []\n","\tdist_validation[str(i + 1)] = []\n","\tdist_test[str(i + 1)] = []\n","\n","for each_site in unique_sites:\n","\tindex_site = get_index(sites, each_site)\n","\tlabel = np.zeros((len(index_site)))\n","\tfor i in range(len(index_site)):\n","\t\tindex = index_site[i]\n","\t\tlabel[i] = int(labels[int(index)])\n","\t########################################### StratifiedKFold ####################################################\n","\tsfolder = StratifiedKFold(n_splits=k_fold,random_state=0,shuffle=True)\n","\tgroup = 0\n","\tfor train, validation in sfolder.split(index_site,label):\n","\t\tfor i in train:\n","\t\t\tdist_train[str(group + 1)].append(index_site[i])\n","\t\t\tname = 0\n","\t\tfor j in validation:\n","\t\t\tdist_validation[str(group + 1)].append(index_site[j])\n","\t\t\tname = 0\n","\t\tgroup = group+1\n","\n","\tgroup = 0\n","\tfor train, validation in sfolder.split(index_site,label):\n","\t\tif group == 0:\n","\t\t\tfor j in validation:\n","\t\t\t\tdist_test[str(group + k_fold)].append(index_site[j])\n","\t\t\t\tdist_train[str(group + k_fold)].remove(index_site[j])\n","\t\telse:\n","\t\t\tfor j in validation:\n","\t\t\t\tdist_test[str(group)].append(index_site[j])\n","\t\t\t\tdist_train[str(group)].remove(index_site[j])\n","\t\tgroup = group+1"]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch_geometric as tg\n","from torch.nn import Linear as Lin, Sequential as Seq\n","############################################### Five models ###############################################################\n","############################################### Five models ###############################################################\n","############################################### Five models ###############################################################\n","# 1. SVM models\n","\n","############################################### FCN model ###############################################################\n","# 2. FCN\n","\n","def single_linear(in_channels, out_channels, dropout):\n","    return nn.Sequential(\n","        nn.Linear(in_features=in_channels, out_features=out_channels),\n","        nn.Dropout(dropout),\n","        nn.ReLU(inplace=True),\n","        nn.BatchNorm1d(out_channels)\n","    )\n","\n","class FCN(nn.Module):\n","    def __init__(self, input_c, hid_1, hid_2, out_c, dropout):\n","        super().__init__()\n","\n","        self.linear_1 = single_linear(input_c, hid_1, dropout)\n","        self.linear_2 = nn.Linear(in_features=hid_1, out_features=hid_2)\n","        self.linear_3 = nn.Linear(in_features=hid_2, out_features=out_c)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","\n","    def forward(self, x):\n","\n","        input = torch.flatten(x, start_dim=1, end_dim=-1)\n","        x = self.relu(self.linear_1(input))\n","        x = self.relu(self.dropout(self.linear_2(x)))\n","        x = self.linear_3(x)\n","        out = self.softmax(x)\n","        return out\n","############################################### AUTO+MLP model ###############################################################\n","# 3. AUTO+MLP\n","\n","class Stacked_autoencoder(nn.Module):\n","\tdef __init__(self, in_c, hid_1, hid_2, hid_3, out, dropout_rate):\n","\n","\t\tsuper(Stacked_autoencoder, self).__init__()\n","\t\tself.linear_1 = nn.Linear(in_features=in_c, out_features=hid_1)\n","\t\tself.linear_2 = nn.Linear(in_features=hid_1, out_features=hid_2)\n","\t\tself.linear_3 = nn.Linear(in_features=hid_2, out_features=hid_3)\n","\t\tself.linear_4 = nn.Linear(in_features=hid_3, out_features=out)\n","\t\tself.relu = nn.ReLU(inplace=True)\n","\t\tself.dropout = nn.Dropout(dropout_rate)\n","\n","\tdef forward(self, data):\n","\n","\t\tinput_data = torch.flatten(data, start_dim=1, end_dim=-1)\n","\t\tlin1 = self.relu(self.dropout(self.linear_1(input_data)))\n","\t\tlin2 = self.relu(self.linear_2(lin1))\n","\t\tlin3 = self.relu(self.linear_3(lin2))\n","\t\tlin4 = self.relu(self.linear_4(lin3))\n","\n","\t\treturn lin4\n","\n","class MLP(nn.Module):\n","\tdef __init__(self, in_c, hid_1, hid_2, out, dropout_rate):\n","\n","\t\tsuper(MLP, self).__init__()\n","\t\tself.linear_1 = nn.Linear(in_features=in_c, out_features=hid_1)\n","\t\tself.linear_2 = nn.Linear(in_features=hid_1, out_features=hid_2)\n","\t\tself.linear_3 = nn.Linear(in_features=hid_2, out_features=out)\n","\t\tself.relu = nn.ReLU(inplace=True)\n","\t\tself.dropout = nn.Dropout(dropout_rate)\n","\n","\tdef forward(self, data):\n","\n","\t\tlin1 = self.relu(self.linear_1(data))\n","\t\tlin2 = self.dropout(self.relu(self.linear_2(lin1)))\n","\t\tlin3 = self.linear_3(lin2)\n","\n","\t\treturn lin3\n","\n","\n","class Auto_encoder_MLP(nn.Module):\n","\tdef __init__(self, in_c, auto_1, auto_2, auto_3, MLP_1, MLP_2, MLP_out, dropout_rate):\n","\n","\t\tsuper(Auto_encoder_MLP, self).__init__()\n","\t\tself.auto_encoder = Stacked_autoencoder(in_c, auto_1, auto_2, auto_3, in_c, dropout_rate)\n","\t\tself.MLP = MLP(in_c, MLP_1, MLP_2, MLP_out, dropout_rate)\n","\t\tself.softmax = nn.LogSoftmax(dim=1)\n","\n","\tdef forward(self, data):\n","\n","\t\tauto_output = self.auto_encoder(data)\n","\t\tMLP_out = self.MLP(auto_output)\n","\t\tout = self.softmax(MLP_out)\n","\n","\t\treturn out, auto_output\n","############################################### GCN model ###############################################################\n","# 4. GCN\n","\n","class GCN(nn.Module):\n","    def __init__(self, in_c, hid_c, out_c, K, dropout_rate, normalize=True):\n","        \"\"\"\n","        :param in_c: int, number of input channels.\n","        :param hid_c: int, number of hidden channels.\n","        :param out_c: int, number of output channels.\n","        :param K:\n","        \"\"\"\n","        super(GCN, self).__init__()\n","        self.normalize = normalize\n","\n","        self.conv1 = tg.nn.ChebConv(in_c, hid_c, K, normalization='sym', bias=True)\n","        self.conv2 = tg.nn.ChebConv(hid_c, hid_c, K, normalization='sym', bias=True)\n","        self.conv3 = tg.nn.ChebConv(hid_c, out_c, K, normalization='sym', bias=True)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","\n","    def forward(self, data, edge_index, edgenet_input):\n","\n","        edge_weight = torch.squeeze(edgenet_input)\n","        # data = self.dropout(data)\n","        h = self.relu(self.dropout(self.conv1(data, edge_index, edge_weight)))\n","        h = self.relu(self.dropout(self.conv2(h, edge_index, edge_weight)))\n","        h = self.conv3(h, edge_index, edge_weight)\n","\n","        return self.softmax(h)\n","############################################### EV-GCN ###############################################################\n","# 5. EV_GCN\n","\n","class PAE(torch.nn.Module):\n","    def __init__(self, input_dim, dropout=0.2):\n","        super(PAE, self).__init__()\n","        hidden=128\n","        self.parser =nn.Sequential(\n","                nn.Linear(input_dim, hidden, bias=True),\n","                nn.ReLU(inplace=True),\n","                nn.BatchNorm1d(hidden),\n","                nn.Dropout(dropout),\n","                nn.Linear(hidden, hidden, bias=True),\n","                )\n","        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n","        self.input_dim = input_dim\n","        self.model_init()\n","        self.relu = nn.ReLU(inplace=True)\n","        self.elu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x1 = x[:,0:self.input_dim]\n","        x2 = x[:,self.input_dim:]\n","        h1 = self.parser(x1)\n","        h2 = self.parser(x2)\n","        p = (self.cos(h1,h2) + 1)*0.5\n","        return p\n","\n","    def model_init(self):\n","        for m in self.modules():\n","            if isinstance(m, Lin):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","                m.weight.requires_grad = True\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","                    m.bias.requires_grad = True\n","\n","\n","class EV_GCN(torch.nn.Module):\n","    def __init__(self, input_dim, num_classes, dropout, edgenet_input_dim, edge_dropout, hgc, lg):\n","        super(EV_GCN, self).__init__()\n","        K=2\n","        hidden = [hgc for i in range(lg)]\n","        self.dropout = dropout \n","        self.edge_dropout = edge_dropout  \n","        bias = False \n","        self.relu = torch.nn.ReLU(inplace=True) \n","        self.lg = lg \n","        self.gconv = nn.ModuleList()\n","        for i in range(lg): \n","            in_channels = input_dim if i==0  else hidden[i-1]\n","            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n","        cls_input_dim = sum(hidden) \n","\n","        self.cls = nn.Sequential(\n","                torch.nn.Linear(cls_input_dim, 256),\n","                torch.nn.ReLU(inplace=True),\n","                nn.BatchNorm1d(256), \n","                torch.nn.Linear(256, num_classes))\n","\n","        self.edge_net = PAE(input_dim=edgenet_input_dim//2, dropout=dropout)\n","        self.model_init()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def model_init(self):\n","        for m in self.modules():\n","            if isinstance(m, Lin):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","                m.weight.requires_grad = True\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","                    m.bias.requires_grad = True\n","\n","    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n","        if self.edge_dropout>0:  \n","            if enforce_edropout or self.training: \n","                one_mask = torch.ones([edgenet_input.shape[0],1]).cuda() \n","                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n","                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n","                edge_index = edge_index[:, self.bool_mask]\n","                edgenet_input = edgenet_input[self.bool_mask] \n","\n","\n","        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n","        features = F.dropout(features, self.dropout, self.training)\n","        h = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n","        h0 = h\n","        for i in range(1, self.lg): \n","            h = F.dropout(h, self.dropout, self.training)\n","            h= self.relu(self.gconv[i](h, edge_index, edge_weight)) \n","            jk = torch.cat((h0, h), axis=1)\n","            h0 = jk\n","        logit = self.cls(jk)\n","\n","\n","        return self.softmax(logit), edge_weight"],"metadata":{"id":"kVsOGdbZd93K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.spatial import distance\n","\n","def get_subject_score(subject_list,l, path, all_subject_list):\n","\n","    name = l + '.mat'\n","    file = scio.loadmat(os.path.join(path, name))\n","    file = file[l]\n","    label_dict = {}\n","    for i in subject_list:\n","        sample_index = get_index(lst=all_subject_list, item=i)\n","        value = file[sample_index[0]]\n","        if l == 'genders':\n","            label_dict[i] = int(value)\n","        elif l == 'ages':\n","            label_dict[i] = float(value)\n","        elif l == 'FIQS':\n","            label_dict[i] = float(value)\n","        elif l == 'NUM':\n","            label_dict[i] = int(value)\n","        elif l == 'PEC':\n","            label_dict[i] = float(value)\n","        elif l == 'RAT':\n","            label_dict[i] = int(value)\n","        elif l == 'sites':\n","            label_dict[i] = value.replace(' ', '')\n","        else:\n","            label_dict[i] = value\n","    return label_dict\n","\n","def create_graph_from_scores(scores, subject_list, path, all_subject_list):\n","    \"\"\"\n","        scores       : list of phenotypic information to be used to construct the affinity graph\n","        subject_list : list of subject IDs\n","    return:\n","        graph        : adjacency matrix of the population graph (num_subjects x num_subjects)\n","    \"\"\"\n","\n","    num_nodes = len(subject_list)\n","    graph = np.zeros((num_nodes, num_nodes))\n","\n","    for l in scores:\n","        label_dict = get_subject_score(subject_list, l, path, all_subject_list)\n","\n","        # quantitative phenotypic scores\n","        if l in ['ages']:\n","            for k in range(num_nodes):\n","                for j in range(k + 1, num_nodes):\n","                    try:\n","                        val = abs(float(label_dict[subject_list[k]]) - float(label_dict[subject_list[j]]))\n","                        if val < 2:\n","                            graph[k, j] += 1\n","                            graph[j, k] += 1\n","                    except ValueError:  # missing label\n","                        pass\n","        elif l in ['FIQ']:\n","            for k in range(num_nodes):\n","                for j in range(k + 1, num_nodes):\n","                    try:\n","                        val = abs(float(label_dict[subject_list[k]]) - float(label_dict[subject_list[j]]))\n","                        if val < 10:\n","                            graph[k, j] += 1\n","                            graph[j, k] += 1\n","                    except ValueError:  # missing label\n","                        pass\n","\n","        else:\n","            for k in range(num_nodes):\n","                for j in range(k + 1, num_nodes):\n","                    if label_dict[subject_list[k]] == label_dict[subject_list[j]]:\n","                        graph[k, j] += 1\n","                        graph[j, k] += 1\n","\n","    return graph\n","\n","\n","def EV_GCN_create_affinity_graph_from_scores(scores, pd_dict):\n","    num_nodes = len(pd_dict[scores[0]]) \n","    graph = np.zeros((num_nodes, num_nodes))\n","\n","    for l in scores:\n","        label_dict = pd_dict[l]\n","\n","        if l in ['AGE_AT_SCAN', 'FIQ']:\n","            for k in range(num_nodes):\n","                for j in range(k + 1, num_nodes):\n","                    try:\n","                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n","                        if val < 2:\n","                            graph[k, j] += 1\n","                            graph[j, k] += 1\n","                    except ValueError:  # missing label\n","                        pass\n","\n","        else:\n","            for k in range(num_nodes):\n","                for j in range(k + 1, num_nodes):\n","                    if label_dict[k] == label_dict[j]:\n","                        graph[k, j] += 1\n","                        graph[j, k] += 1\n","\n","    return graph\n","\n","def EV_GCN_get_static_affinity_adj(features, pd_dict):\n","    pd_affinity = EV_GCN_create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n","    distv = distance.pdist(features, metric='correlation') \n","    dist = distance.squareform(distv)  \n","    sigma = np.mean(dist)\n","    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n","    adj = pd_affinity * feature_sim  \n","\n","    return adj\n","\n","\n","\n","all_subject_IDs = get_ids(number_samples,dir_path=label_dir)\n","all_subject_IDs = all_subject_IDs.tolist()\n","for i in useless_samples:\n","    subject_IDs.remove(i)\n","\n","graph = create_graph_from_scores(['sites'], subject_IDs, label_dir,all_subject_IDs)\n","\n","\n","\n","selectors = {}\n","ALL_labels = np.zeros((number_samples - len(useless_samples), 2))\n","for i in range(len(labels)):\n","    if labels[i] == 1:\n","        ALL_labels[i,0] = 1\n","    else:\n","        ALL_labels[i,1] = 1\n","\n","FCN_data = {}\n","GCN_data = {}\n","EV_GCN_data = {}\n","SVM_data = {}\n","for i in range(0, k_fold):\n","  selectors[str(i+1)] = feature_selection(fMRI_images, labels, dist_train[str(i+1)], new_number_features)\n","  fold_data = selectors[str(i+1)].transform(fMRI_images)\n","  FCN_data[str(i+1)] = []\n","  FCN_data[str(i+1)].append(fold_data)\n","  FCN_data[str(i+1)].append(ALL_labels)\n","  SVM_data[str(i+1)] = fold_data\n","######################################################### GCN ###########################################################\n","  n = fold_data.shape[0]\n","  num_edge = n * n\n","  edge_index = np.zeros([2, num_edge], dtype=np.int64)\n","  edgenet_input = np.zeros([num_edge, 1], dtype=np.float32) \n","  aff_score = np.zeros(num_edge, dtype=np.float32)\n","  flatten_ind = 0\n","\n","  for x in range(n): \n","    for y in range(n):\n","      edge_index[:, flatten_ind] = [x, y]\n","      edgenet_input[flatten_ind] = graph[x,y]\n","      aff_score[flatten_ind] = graph[x,y]\n","      flatten_ind += 1\n","\n","\n","  keep_ind = np.where(aff_score > 0.99)[0]\n","  edge_index = edge_index[:, keep_ind]\n","  edgenet_input = edgenet_input[keep_ind]\n","  GCN_data[str(i+1)] = []\n","  GCN_data[str(i+1)].append(fold_data)\n","  GCN_data[str(i+1)].append(ALL_labels)\n","  GCN_data[str(i+1)].append(edge_index)\n","  GCN_data[str(i+1)].append(edgenet_input)\n","\n","######################################################### EV_GCN ###########################################################\n","  n = fold_data.shape[0]\n","  num_edge = n * n\n","  edge_index = np.zeros([2, num_edge], dtype=np.int64)\n","  edgenet_input = np.zeros([num_edge, 2*phonetic_data.shape[1]], dtype=np.float32) \n","  aff_score = np.zeros(num_edge, dtype=np.float32)\n","  aff_adj = EV_GCN_get_static_affinity_adj(fold_data, pd_dict)  \n","\n","  flatten_ind = 0\n","  for x in range(n): \n","    for y in range(n):\n","      edge_index[:, flatten_ind] = [x, y]\n","      edgenet_input[flatten_ind] = np.concatenate((phonetic_data[x], phonetic_data[y]))\n","      aff_score[flatten_ind] = aff_adj[x][y]\n","      flatten_ind += 1\n","\n","\n","  keep_ind = np.where(aff_score > 1.1)[0]\n","  edge_index = edge_index[:, keep_ind]\n","  edgenet_input = edgenet_input[keep_ind]\n","  edgenet_input = (edgenet_input- edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n","\n","  EV_GCN_data[str(i+1)] = []\n","  EV_GCN_data[str(i+1)].append(fold_data)\n","  EV_GCN_data[str(i+1)].append(ALL_labels)\n","  EV_GCN_data[str(i+1)].append(edge_index)\n","  EV_GCN_data[str(i+1)].append(edgenet_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hj5zN8vkiPrR","executionInfo":{"status":"ok","timestamp":1670448312442,"user_tz":0,"elapsed":357127,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"b8732d41-66af-4f24-f198-dd0164aec301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting estimator with 19900 features.\n","Fitting estimator with 19800 features.\n","Fitting estimator with 19700 features.\n","Fitting estimator with 19600 features.\n","Fitting estimator with 19500 features.\n","Fitting estimator with 19400 features.\n","Fitting estimator with 19300 features.\n","Fitting estimator with 19200 features.\n","Fitting estimator with 19100 features.\n","Fitting estimator with 19000 features.\n","Fitting estimator with 18900 features.\n","Fitting estimator with 18800 features.\n","Fitting estimator with 18700 features.\n","Fitting estimator with 18600 features.\n","Fitting estimator with 18500 features.\n","Fitting estimator with 18400 features.\n","Fitting estimator with 18300 features.\n","Fitting estimator with 18200 features.\n","Fitting estimator with 18100 features.\n","Fitting estimator with 18000 features.\n","Fitting estimator with 17900 features.\n","Fitting estimator with 17800 features.\n","Fitting estimator with 17700 features.\n","Fitting estimator with 17600 features.\n","Fitting estimator with 17500 features.\n","Fitting estimator with 17400 features.\n","Fitting estimator with 17300 features.\n","Fitting estimator with 17200 features.\n","Fitting estimator with 17100 features.\n","Fitting estimator with 17000 features.\n","Fitting estimator with 16900 features.\n","Fitting estimator with 16800 features.\n","Fitting estimator with 16700 features.\n","Fitting estimator with 16600 features.\n","Fitting estimator with 16500 features.\n","Fitting estimator with 16400 features.\n","Fitting estimator with 16300 features.\n","Fitting estimator with 16200 features.\n","Fitting estimator with 16100 features.\n","Fitting estimator with 16000 features.\n","Fitting estimator with 15900 features.\n","Fitting estimator with 15800 features.\n","Fitting estimator with 15700 features.\n","Fitting estimator with 15600 features.\n","Fitting estimator with 15500 features.\n","Fitting estimator with 15400 features.\n","Fitting estimator with 15300 features.\n","Fitting estimator with 15200 features.\n","Fitting estimator with 15100 features.\n","Fitting estimator with 15000 features.\n","Fitting estimator with 14900 features.\n","Fitting estimator with 14800 features.\n","Fitting estimator with 14700 features.\n","Fitting estimator with 14600 features.\n","Fitting estimator with 14500 features.\n","Fitting estimator with 14400 features.\n","Fitting estimator with 14300 features.\n","Fitting estimator with 14200 features.\n","Fitting estimator with 14100 features.\n","Fitting estimator with 14000 features.\n","Fitting estimator with 13900 features.\n","Fitting estimator with 13800 features.\n","Fitting estimator with 13700 features.\n","Fitting estimator with 13600 features.\n","Fitting estimator with 13500 features.\n","Fitting estimator with 13400 features.\n","Fitting estimator with 13300 features.\n","Fitting estimator with 13200 features.\n","Fitting estimator with 13100 features.\n","Fitting estimator with 13000 features.\n","Fitting estimator with 12900 features.\n","Fitting estimator with 12800 features.\n","Fitting estimator with 12700 features.\n","Fitting estimator with 12600 features.\n","Fitting estimator with 12500 features.\n","Fitting estimator with 12400 features.\n","Fitting estimator with 12300 features.\n","Fitting estimator with 12200 features.\n","Fitting estimator with 12100 features.\n","Fitting estimator with 12000 features.\n","Fitting estimator with 11900 features.\n","Fitting estimator with 11800 features.\n","Fitting estimator with 11700 features.\n","Fitting estimator with 11600 features.\n","Fitting estimator with 11500 features.\n","Fitting estimator with 11400 features.\n","Fitting estimator with 11300 features.\n","Fitting estimator with 11200 features.\n","Fitting estimator with 11100 features.\n","Fitting estimator with 11000 features.\n","Fitting estimator with 10900 features.\n","Fitting estimator with 10800 features.\n","Fitting estimator with 10700 features.\n","Fitting estimator with 10600 features.\n","Fitting estimator with 10500 features.\n","Fitting estimator with 10400 features.\n","Fitting estimator with 10300 features.\n","Fitting estimator with 10200 features.\n","Fitting estimator with 10100 features.\n","Fitting estimator with 10000 features.\n","Fitting estimator with 9900 features.\n","Fitting estimator with 9800 features.\n","Fitting estimator with 9700 features.\n","Fitting estimator with 9600 features.\n","Fitting estimator with 9500 features.\n","Fitting estimator with 9400 features.\n","Fitting estimator with 9300 features.\n","Fitting estimator with 9200 features.\n","Fitting estimator with 9100 features.\n","Fitting estimator with 9000 features.\n","Fitting estimator with 8900 features.\n","Fitting estimator with 8800 features.\n","Fitting estimator with 8700 features.\n","Fitting estimator with 8600 features.\n","Fitting estimator with 8500 features.\n","Fitting estimator with 8400 features.\n","Fitting estimator with 8300 features.\n","Fitting estimator with 8200 features.\n","Fitting estimator with 8100 features.\n","Fitting estimator with 8000 features.\n","Fitting estimator with 7900 features.\n","Fitting estimator with 7800 features.\n","Fitting estimator with 7700 features.\n","Fitting estimator with 7600 features.\n","Fitting estimator with 7500 features.\n","Fitting estimator with 7400 features.\n","Fitting estimator with 7300 features.\n","Fitting estimator with 7200 features.\n","Fitting estimator with 7100 features.\n","Fitting estimator with 7000 features.\n","Fitting estimator with 6900 features.\n","Fitting estimator with 6800 features.\n","Fitting estimator with 6700 features.\n","Fitting estimator with 6600 features.\n","Fitting estimator with 6500 features.\n","Fitting estimator with 6400 features.\n","Fitting estimator with 6300 features.\n","Fitting estimator with 6200 features.\n","Fitting estimator with 6100 features.\n","Fitting estimator with 6000 features.\n","Fitting estimator with 5900 features.\n","Fitting estimator with 5800 features.\n","Fitting estimator with 5700 features.\n","Fitting estimator with 5600 features.\n","Fitting estimator with 5500 features.\n","Fitting estimator with 5400 features.\n","Fitting estimator with 5300 features.\n","Fitting estimator with 5200 features.\n","Fitting estimator with 5100 features.\n","Fitting estimator with 19900 features.\n","Fitting estimator with 19800 features.\n","Fitting estimator with 19700 features.\n","Fitting estimator with 19600 features.\n","Fitting estimator with 19500 features.\n","Fitting estimator with 19400 features.\n","Fitting estimator with 19300 features.\n","Fitting estimator with 19200 features.\n","Fitting estimator with 19100 features.\n","Fitting estimator with 19000 features.\n","Fitting estimator with 18900 features.\n","Fitting estimator with 18800 features.\n","Fitting estimator with 18700 features.\n","Fitting estimator with 18600 features.\n","Fitting estimator with 18500 features.\n","Fitting estimator with 18400 features.\n","Fitting estimator with 18300 features.\n","Fitting estimator with 18200 features.\n","Fitting estimator with 18100 features.\n","Fitting estimator with 18000 features.\n","Fitting estimator with 17900 features.\n","Fitting estimator with 17800 features.\n","Fitting estimator with 17700 features.\n","Fitting estimator with 17600 features.\n","Fitting estimator with 17500 features.\n","Fitting estimator with 17400 features.\n","Fitting estimator with 17300 features.\n","Fitting estimator with 17200 features.\n","Fitting estimator with 17100 features.\n","Fitting estimator with 17000 features.\n","Fitting estimator with 16900 features.\n","Fitting estimator with 16800 features.\n","Fitting estimator with 16700 features.\n","Fitting estimator with 16600 features.\n","Fitting estimator with 16500 features.\n","Fitting estimator with 16400 features.\n","Fitting estimator with 16300 features.\n","Fitting estimator with 16200 features.\n","Fitting estimator with 16100 features.\n","Fitting estimator with 16000 features.\n","Fitting estimator with 15900 features.\n","Fitting estimator with 15800 features.\n","Fitting estimator with 15700 features.\n","Fitting estimator with 15600 features.\n","Fitting estimator with 15500 features.\n","Fitting estimator with 15400 features.\n","Fitting estimator with 15300 features.\n","Fitting estimator with 15200 features.\n","Fitting estimator with 15100 features.\n","Fitting estimator with 15000 features.\n","Fitting estimator with 14900 features.\n","Fitting estimator with 14800 features.\n","Fitting estimator with 14700 features.\n","Fitting estimator with 14600 features.\n","Fitting estimator with 14500 features.\n","Fitting estimator with 14400 features.\n","Fitting estimator with 14300 features.\n","Fitting estimator with 14200 features.\n","Fitting estimator with 14100 features.\n","Fitting estimator with 14000 features.\n","Fitting estimator with 13900 features.\n","Fitting estimator with 13800 features.\n","Fitting estimator with 13700 features.\n","Fitting estimator with 13600 features.\n","Fitting estimator with 13500 features.\n","Fitting estimator with 13400 features.\n","Fitting estimator with 13300 features.\n","Fitting estimator with 13200 features.\n","Fitting estimator with 13100 features.\n","Fitting estimator with 13000 features.\n","Fitting estimator with 12900 features.\n","Fitting estimator with 12800 features.\n","Fitting estimator with 12700 features.\n","Fitting estimator with 12600 features.\n","Fitting estimator with 12500 features.\n","Fitting estimator with 12400 features.\n","Fitting estimator with 12300 features.\n","Fitting estimator with 12200 features.\n","Fitting estimator with 12100 features.\n","Fitting estimator with 12000 features.\n","Fitting estimator with 11900 features.\n","Fitting estimator with 11800 features.\n","Fitting estimator with 11700 features.\n","Fitting estimator with 11600 features.\n","Fitting estimator with 11500 features.\n","Fitting estimator with 11400 features.\n","Fitting estimator with 11300 features.\n","Fitting estimator with 11200 features.\n","Fitting estimator with 11100 features.\n","Fitting estimator with 11000 features.\n","Fitting estimator with 10900 features.\n","Fitting estimator with 10800 features.\n","Fitting estimator with 10700 features.\n","Fitting estimator with 10600 features.\n","Fitting estimator with 10500 features.\n","Fitting estimator with 10400 features.\n","Fitting estimator with 10300 features.\n","Fitting estimator with 10200 features.\n","Fitting estimator with 10100 features.\n","Fitting estimator with 10000 features.\n","Fitting estimator with 9900 features.\n","Fitting estimator with 9800 features.\n","Fitting estimator with 9700 features.\n","Fitting estimator with 9600 features.\n","Fitting estimator with 9500 features.\n","Fitting estimator with 9400 features.\n","Fitting estimator with 9300 features.\n","Fitting estimator with 9200 features.\n","Fitting estimator with 9100 features.\n","Fitting estimator with 9000 features.\n","Fitting estimator with 8900 features.\n","Fitting estimator with 8800 features.\n","Fitting estimator with 8700 features.\n","Fitting estimator with 8600 features.\n","Fitting estimator with 8500 features.\n","Fitting estimator with 8400 features.\n","Fitting estimator with 8300 features.\n","Fitting estimator with 8200 features.\n","Fitting estimator with 8100 features.\n","Fitting estimator with 8000 features.\n","Fitting estimator with 7900 features.\n","Fitting estimator with 7800 features.\n","Fitting estimator with 7700 features.\n","Fitting estimator with 7600 features.\n","Fitting estimator with 7500 features.\n","Fitting estimator with 7400 features.\n","Fitting estimator with 7300 features.\n","Fitting estimator with 7200 features.\n","Fitting estimator with 7100 features.\n","Fitting estimator with 7000 features.\n","Fitting estimator with 6900 features.\n","Fitting estimator with 6800 features.\n","Fitting estimator with 6700 features.\n","Fitting estimator with 6600 features.\n","Fitting estimator with 6500 features.\n","Fitting estimator with 6400 features.\n","Fitting estimator with 6300 features.\n","Fitting estimator with 6200 features.\n","Fitting estimator with 6100 features.\n","Fitting estimator with 6000 features.\n","Fitting estimator with 5900 features.\n","Fitting estimator with 5800 features.\n","Fitting estimator with 5700 features.\n","Fitting estimator with 5600 features.\n","Fitting estimator with 5500 features.\n","Fitting estimator with 5400 features.\n","Fitting estimator with 5300 features.\n","Fitting estimator with 5200 features.\n","Fitting estimator with 5100 features.\n","Fitting estimator with 19900 features.\n","Fitting estimator with 19800 features.\n","Fitting estimator with 19700 features.\n","Fitting estimator with 19600 features.\n","Fitting estimator with 19500 features.\n","Fitting estimator with 19400 features.\n","Fitting estimator with 19300 features.\n","Fitting estimator with 19200 features.\n","Fitting estimator with 19100 features.\n","Fitting estimator with 19000 features.\n","Fitting estimator with 18900 features.\n","Fitting estimator with 18800 features.\n","Fitting estimator with 18700 features.\n","Fitting estimator with 18600 features.\n","Fitting estimator with 18500 features.\n","Fitting estimator with 18400 features.\n","Fitting estimator with 18300 features.\n","Fitting estimator with 18200 features.\n","Fitting estimator with 18100 features.\n","Fitting estimator with 18000 features.\n","Fitting estimator with 17900 features.\n","Fitting estimator with 17800 features.\n","Fitting estimator with 17700 features.\n","Fitting estimator with 17600 features.\n","Fitting estimator with 17500 features.\n","Fitting estimator with 17400 features.\n","Fitting estimator with 17300 features.\n","Fitting estimator with 17200 features.\n","Fitting estimator with 17100 features.\n","Fitting estimator with 17000 features.\n","Fitting estimator with 16900 features.\n","Fitting estimator with 16800 features.\n","Fitting estimator with 16700 features.\n","Fitting estimator with 16600 features.\n","Fitting estimator with 16500 features.\n","Fitting estimator with 16400 features.\n","Fitting estimator with 16300 features.\n","Fitting estimator with 16200 features.\n","Fitting estimator with 16100 features.\n","Fitting estimator with 16000 features.\n","Fitting estimator with 15900 features.\n","Fitting estimator with 15800 features.\n","Fitting estimator with 15700 features.\n","Fitting estimator with 15600 features.\n","Fitting estimator with 15500 features.\n","Fitting estimator with 15400 features.\n","Fitting estimator with 15300 features.\n","Fitting estimator with 15200 features.\n","Fitting estimator with 15100 features.\n","Fitting estimator with 15000 features.\n","Fitting estimator with 14900 features.\n","Fitting estimator with 14800 features.\n","Fitting estimator with 14700 features.\n","Fitting estimator with 14600 features.\n","Fitting estimator with 14500 features.\n","Fitting estimator with 14400 features.\n","Fitting estimator with 14300 features.\n","Fitting estimator with 14200 features.\n","Fitting estimator with 14100 features.\n","Fitting estimator with 14000 features.\n","Fitting estimator with 13900 features.\n","Fitting estimator with 13800 features.\n","Fitting estimator with 13700 features.\n","Fitting estimator with 13600 features.\n","Fitting estimator with 13500 features.\n","Fitting estimator with 13400 features.\n","Fitting estimator with 13300 features.\n","Fitting estimator with 13200 features.\n","Fitting estimator with 13100 features.\n","Fitting estimator with 13000 features.\n","Fitting estimator with 12900 features.\n","Fitting estimator with 12800 features.\n","Fitting estimator with 12700 features.\n","Fitting estimator with 12600 features.\n","Fitting estimator with 12500 features.\n","Fitting estimator with 12400 features.\n","Fitting estimator with 12300 features.\n","Fitting estimator with 12200 features.\n","Fitting estimator with 12100 features.\n","Fitting estimator with 12000 features.\n","Fitting estimator with 11900 features.\n","Fitting estimator with 11800 features.\n","Fitting estimator with 11700 features.\n","Fitting estimator with 11600 features.\n","Fitting estimator with 11500 features.\n","Fitting estimator with 11400 features.\n","Fitting estimator with 11300 features.\n","Fitting estimator with 11200 features.\n","Fitting estimator with 11100 features.\n","Fitting estimator with 11000 features.\n","Fitting estimator with 10900 features.\n","Fitting estimator with 10800 features.\n","Fitting estimator with 10700 features.\n","Fitting estimator with 10600 features.\n","Fitting estimator with 10500 features.\n","Fitting estimator with 10400 features.\n","Fitting estimator with 10300 features.\n","Fitting estimator with 10200 features.\n","Fitting estimator with 10100 features.\n","Fitting estimator with 10000 features.\n","Fitting estimator with 9900 features.\n","Fitting estimator with 9800 features.\n","Fitting estimator with 9700 features.\n","Fitting estimator with 9600 features.\n","Fitting estimator with 9500 features.\n","Fitting estimator with 9400 features.\n","Fitting estimator with 9300 features.\n","Fitting estimator with 9200 features.\n","Fitting estimator with 9100 features.\n","Fitting estimator with 9000 features.\n","Fitting estimator with 8900 features.\n","Fitting estimator with 8800 features.\n","Fitting estimator with 8700 features.\n","Fitting estimator with 8600 features.\n","Fitting estimator with 8500 features.\n","Fitting estimator with 8400 features.\n","Fitting estimator with 8300 features.\n","Fitting estimator with 8200 features.\n","Fitting estimator with 8100 features.\n","Fitting estimator with 8000 features.\n","Fitting estimator with 7900 features.\n","Fitting estimator with 7800 features.\n","Fitting estimator with 7700 features.\n","Fitting estimator with 7600 features.\n","Fitting estimator with 7500 features.\n","Fitting estimator with 7400 features.\n","Fitting estimator with 7300 features.\n","Fitting estimator with 7200 features.\n","Fitting estimator with 7100 features.\n","Fitting estimator with 7000 features.\n","Fitting estimator with 6900 features.\n","Fitting estimator with 6800 features.\n","Fitting estimator with 6700 features.\n","Fitting estimator with 6600 features.\n","Fitting estimator with 6500 features.\n","Fitting estimator with 6400 features.\n","Fitting estimator with 6300 features.\n","Fitting estimator with 6200 features.\n","Fitting estimator with 6100 features.\n","Fitting estimator with 6000 features.\n","Fitting estimator with 5900 features.\n","Fitting estimator with 5800 features.\n","Fitting estimator with 5700 features.\n","Fitting estimator with 5600 features.\n","Fitting estimator with 5500 features.\n","Fitting estimator with 5400 features.\n","Fitting estimator with 5300 features.\n","Fitting estimator with 5200 features.\n","Fitting estimator with 5100 features.\n","Fitting estimator with 19900 features.\n","Fitting estimator with 19800 features.\n","Fitting estimator with 19700 features.\n","Fitting estimator with 19600 features.\n","Fitting estimator with 19500 features.\n","Fitting estimator with 19400 features.\n","Fitting estimator with 19300 features.\n","Fitting estimator with 19200 features.\n","Fitting estimator with 19100 features.\n","Fitting estimator with 19000 features.\n","Fitting estimator with 18900 features.\n","Fitting estimator with 18800 features.\n","Fitting estimator with 18700 features.\n","Fitting estimator with 18600 features.\n","Fitting estimator with 18500 features.\n","Fitting estimator with 18400 features.\n","Fitting estimator with 18300 features.\n","Fitting estimator with 18200 features.\n","Fitting estimator with 18100 features.\n","Fitting estimator with 18000 features.\n","Fitting estimator with 17900 features.\n","Fitting estimator with 17800 features.\n","Fitting estimator with 17700 features.\n","Fitting estimator with 17600 features.\n","Fitting estimator with 17500 features.\n","Fitting estimator with 17400 features.\n","Fitting estimator with 17300 features.\n","Fitting estimator with 17200 features.\n","Fitting estimator with 17100 features.\n","Fitting estimator with 17000 features.\n","Fitting estimator with 16900 features.\n","Fitting estimator with 16800 features.\n","Fitting estimator with 16700 features.\n","Fitting estimator with 16600 features.\n","Fitting estimator with 16500 features.\n","Fitting estimator with 16400 features.\n","Fitting estimator with 16300 features.\n","Fitting estimator with 16200 features.\n","Fitting estimator with 16100 features.\n","Fitting estimator with 16000 features.\n","Fitting estimator with 15900 features.\n","Fitting estimator with 15800 features.\n","Fitting estimator with 15700 features.\n","Fitting estimator with 15600 features.\n","Fitting estimator with 15500 features.\n","Fitting estimator with 15400 features.\n","Fitting estimator with 15300 features.\n","Fitting estimator with 15200 features.\n","Fitting estimator with 15100 features.\n","Fitting estimator with 15000 features.\n","Fitting estimator with 14900 features.\n","Fitting estimator with 14800 features.\n","Fitting estimator with 14700 features.\n","Fitting estimator with 14600 features.\n","Fitting estimator with 14500 features.\n","Fitting estimator with 14400 features.\n","Fitting estimator with 14300 features.\n","Fitting estimator with 14200 features.\n","Fitting estimator with 14100 features.\n","Fitting estimator with 14000 features.\n","Fitting estimator with 13900 features.\n","Fitting estimator with 13800 features.\n","Fitting estimator with 13700 features.\n","Fitting estimator with 13600 features.\n","Fitting estimator with 13500 features.\n","Fitting estimator with 13400 features.\n","Fitting estimator with 13300 features.\n","Fitting estimator with 13200 features.\n","Fitting estimator with 13100 features.\n","Fitting estimator with 13000 features.\n","Fitting estimator with 12900 features.\n","Fitting estimator with 12800 features.\n","Fitting estimator with 12700 features.\n","Fitting estimator with 12600 features.\n","Fitting estimator with 12500 features.\n","Fitting estimator with 12400 features.\n","Fitting estimator with 12300 features.\n","Fitting estimator with 12200 features.\n","Fitting estimator with 12100 features.\n","Fitting estimator with 12000 features.\n","Fitting estimator with 11900 features.\n","Fitting estimator with 11800 features.\n","Fitting estimator with 11700 features.\n","Fitting estimator with 11600 features.\n","Fitting estimator with 11500 features.\n","Fitting estimator with 11400 features.\n","Fitting estimator with 11300 features.\n","Fitting estimator with 11200 features.\n","Fitting estimator with 11100 features.\n","Fitting estimator with 11000 features.\n","Fitting estimator with 10900 features.\n","Fitting estimator with 10800 features.\n","Fitting estimator with 10700 features.\n","Fitting estimator with 10600 features.\n","Fitting estimator with 10500 features.\n","Fitting estimator with 10400 features.\n","Fitting estimator with 10300 features.\n","Fitting estimator with 10200 features.\n","Fitting estimator with 10100 features.\n","Fitting estimator with 10000 features.\n","Fitting estimator with 9900 features.\n","Fitting estimator with 9800 features.\n","Fitting estimator with 9700 features.\n","Fitting estimator with 9600 features.\n","Fitting estimator with 9500 features.\n","Fitting estimator with 9400 features.\n","Fitting estimator with 9300 features.\n","Fitting estimator with 9200 features.\n","Fitting estimator with 9100 features.\n","Fitting estimator with 9000 features.\n","Fitting estimator with 8900 features.\n","Fitting estimator with 8800 features.\n","Fitting estimator with 8700 features.\n","Fitting estimator with 8600 features.\n","Fitting estimator with 8500 features.\n","Fitting estimator with 8400 features.\n","Fitting estimator with 8300 features.\n","Fitting estimator with 8200 features.\n","Fitting estimator with 8100 features.\n","Fitting estimator with 8000 features.\n","Fitting estimator with 7900 features.\n","Fitting estimator with 7800 features.\n","Fitting estimator with 7700 features.\n","Fitting estimator with 7600 features.\n","Fitting estimator with 7500 features.\n","Fitting estimator with 7400 features.\n","Fitting estimator with 7300 features.\n","Fitting estimator with 7200 features.\n","Fitting estimator with 7100 features.\n","Fitting estimator with 7000 features.\n","Fitting estimator with 6900 features.\n","Fitting estimator with 6800 features.\n","Fitting estimator with 6700 features.\n","Fitting estimator with 6600 features.\n","Fitting estimator with 6500 features.\n","Fitting estimator with 6400 features.\n","Fitting estimator with 6300 features.\n","Fitting estimator with 6200 features.\n","Fitting estimator with 6100 features.\n","Fitting estimator with 6000 features.\n","Fitting estimator with 5900 features.\n","Fitting estimator with 5800 features.\n","Fitting estimator with 5700 features.\n","Fitting estimator with 5600 features.\n","Fitting estimator with 5500 features.\n","Fitting estimator with 5400 features.\n","Fitting estimator with 5300 features.\n","Fitting estimator with 5200 features.\n","Fitting estimator with 5100 features.\n","Fitting estimator with 19900 features.\n","Fitting estimator with 19800 features.\n","Fitting estimator with 19700 features.\n","Fitting estimator with 19600 features.\n","Fitting estimator with 19500 features.\n","Fitting estimator with 19400 features.\n","Fitting estimator with 19300 features.\n","Fitting estimator with 19200 features.\n","Fitting estimator with 19100 features.\n","Fitting estimator with 19000 features.\n","Fitting estimator with 18900 features.\n","Fitting estimator with 18800 features.\n","Fitting estimator with 18700 features.\n","Fitting estimator with 18600 features.\n","Fitting estimator with 18500 features.\n","Fitting estimator with 18400 features.\n","Fitting estimator with 18300 features.\n","Fitting estimator with 18200 features.\n","Fitting estimator with 18100 features.\n","Fitting estimator with 18000 features.\n","Fitting estimator with 17900 features.\n","Fitting estimator with 17800 features.\n","Fitting estimator with 17700 features.\n","Fitting estimator with 17600 features.\n","Fitting estimator with 17500 features.\n","Fitting estimator with 17400 features.\n","Fitting estimator with 17300 features.\n","Fitting estimator with 17200 features.\n","Fitting estimator with 17100 features.\n","Fitting estimator with 17000 features.\n","Fitting estimator with 16900 features.\n","Fitting estimator with 16800 features.\n","Fitting estimator with 16700 features.\n","Fitting estimator with 16600 features.\n","Fitting estimator with 16500 features.\n","Fitting estimator with 16400 features.\n","Fitting estimator with 16300 features.\n","Fitting estimator with 16200 features.\n","Fitting estimator with 16100 features.\n","Fitting estimator with 16000 features.\n","Fitting estimator with 15900 features.\n","Fitting estimator with 15800 features.\n","Fitting estimator with 15700 features.\n","Fitting estimator with 15600 features.\n","Fitting estimator with 15500 features.\n","Fitting estimator with 15400 features.\n","Fitting estimator with 15300 features.\n","Fitting estimator with 15200 features.\n","Fitting estimator with 15100 features.\n","Fitting estimator with 15000 features.\n","Fitting estimator with 14900 features.\n","Fitting estimator with 14800 features.\n","Fitting estimator with 14700 features.\n","Fitting estimator with 14600 features.\n","Fitting estimator with 14500 features.\n","Fitting estimator with 14400 features.\n","Fitting estimator with 14300 features.\n","Fitting estimator with 14200 features.\n","Fitting estimator with 14100 features.\n","Fitting estimator with 14000 features.\n","Fitting estimator with 13900 features.\n","Fitting estimator with 13800 features.\n","Fitting estimator with 13700 features.\n","Fitting estimator with 13600 features.\n","Fitting estimator with 13500 features.\n","Fitting estimator with 13400 features.\n","Fitting estimator with 13300 features.\n","Fitting estimator with 13200 features.\n","Fitting estimator with 13100 features.\n","Fitting estimator with 13000 features.\n","Fitting estimator with 12900 features.\n","Fitting estimator with 12800 features.\n","Fitting estimator with 12700 features.\n","Fitting estimator with 12600 features.\n","Fitting estimator with 12500 features.\n","Fitting estimator with 12400 features.\n","Fitting estimator with 12300 features.\n","Fitting estimator with 12200 features.\n","Fitting estimator with 12100 features.\n","Fitting estimator with 12000 features.\n","Fitting estimator with 11900 features.\n","Fitting estimator with 11800 features.\n","Fitting estimator with 11700 features.\n","Fitting estimator with 11600 features.\n","Fitting estimator with 11500 features.\n","Fitting estimator with 11400 features.\n","Fitting estimator with 11300 features.\n","Fitting estimator with 11200 features.\n","Fitting estimator with 11100 features.\n","Fitting estimator with 11000 features.\n","Fitting estimator with 10900 features.\n","Fitting estimator with 10800 features.\n","Fitting estimator with 10700 features.\n","Fitting estimator with 10600 features.\n","Fitting estimator with 10500 features.\n","Fitting estimator with 10400 features.\n","Fitting estimator with 10300 features.\n","Fitting estimator with 10200 features.\n","Fitting estimator with 10100 features.\n","Fitting estimator with 10000 features.\n","Fitting estimator with 9900 features.\n","Fitting estimator with 9800 features.\n","Fitting estimator with 9700 features.\n","Fitting estimator with 9600 features.\n","Fitting estimator with 9500 features.\n","Fitting estimator with 9400 features.\n","Fitting estimator with 9300 features.\n","Fitting estimator with 9200 features.\n","Fitting estimator with 9100 features.\n","Fitting estimator with 9000 features.\n","Fitting estimator with 8900 features.\n","Fitting estimator with 8800 features.\n","Fitting estimator with 8700 features.\n","Fitting estimator with 8600 features.\n","Fitting estimator with 8500 features.\n","Fitting estimator with 8400 features.\n","Fitting estimator with 8300 features.\n","Fitting estimator with 8200 features.\n","Fitting estimator with 8100 features.\n","Fitting estimator with 8000 features.\n","Fitting estimator with 7900 features.\n","Fitting estimator with 7800 features.\n","Fitting estimator with 7700 features.\n","Fitting estimator with 7600 features.\n","Fitting estimator with 7500 features.\n","Fitting estimator with 7400 features.\n","Fitting estimator with 7300 features.\n","Fitting estimator with 7200 features.\n","Fitting estimator with 7100 features.\n","Fitting estimator with 7000 features.\n","Fitting estimator with 6900 features.\n","Fitting estimator with 6800 features.\n","Fitting estimator with 6700 features.\n","Fitting estimator with 6600 features.\n","Fitting estimator with 6500 features.\n","Fitting estimator with 6400 features.\n","Fitting estimator with 6300 features.\n","Fitting estimator with 6200 features.\n","Fitting estimator with 6100 features.\n","Fitting estimator with 6000 features.\n","Fitting estimator with 5900 features.\n","Fitting estimator with 5800 features.\n","Fitting estimator with 5700 features.\n","Fitting estimator with 5600 features.\n","Fitting estimator with 5500 features.\n","Fitting estimator with 5400 features.\n","Fitting estimator with 5300 features.\n","Fitting estimator with 5200 features.\n","Fitting estimator with 5100 features.\n"]}]},{"cell_type":"code","source":["############################################### SVM ###############################################################\n","############################################### SVM ###############################################################\n","############################################### SVM ###############################################################\n","average_test = 0\n","for i in range(0, k_fold):\n","\n","  ################################################# feature selection #########################################################\n","  # selector = selectors[str(i+1)]\n","  ###############################################################################################################\n","  model_name = save_path + 'SVM_'+str(i+1) + '.m' \n","  model = joblib.load(model_name)\n","\n","  # test_images = np.zeros((len(dist_test[str(i+1)]), fMRI_images.shape[1]))\n","  test_labels = np.zeros((len(dist_test[str(i + 1)]), 1))\n","\n","  test_images = SVM_data[str(i+1)][dist_test[str(i+1)],:]\n","  test_labels[:, 0] = labels[dist_test[str(i + 1)]]\n","  # test_images = selector.transform(test_images)\n","\n","  print('Model_'+ str(i+1))\n","  test_results = model.predict(test_images)\n","  test_results_prob = model.predict_proba(test_images)\n","\n","  test_accuracy = metrics.accuracy_score(test_labels, test_results)\n","  if i == 0:\n","    test_labels_concat = test_labels\n","    SVM_results_concat = test_results\n","    SVM_results_prob_concat = test_results_prob\n","  else:\n","    test_labels_concat = np.concatenate((test_labels_concat, test_labels),axis = 0)\n","    SVM_results_concat = np.concatenate((SVM_results_concat, test_results),axis = 0)\n","    SVM_results_prob_concat = np.concatenate((SVM_results_prob_concat, test_results_prob),axis = 0)\n","\n","\n","test_labels_concat = np.squeeze(test_labels_concat)\n","test_accuracy = metrics.accuracy_score(test_labels_concat, SVM_results_concat)\n","print(test_accuracy)\n","print(test_labels_concat)\n","print(SVM_results_concat)\n","print(SVM_results_prob_concat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lf75vhbfDbmx","executionInfo":{"status":"ok","timestamp":1670448319935,"user_tz":0,"elapsed":7496,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"7a65fdef-4ec7-4c12-be3d-f703d4c7471e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model_1\n","Model_2\n","Model_3\n","Model_4\n","Model_5\n","0.6735632183908046\n","[1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n"," 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n"," 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1.]\n","[1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n"," 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.\n"," 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n"," 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n"," 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n"," 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n"," 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n"," 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n"," 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n"," 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.\n"," 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1.\n"," 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1.\n"," 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1.\n"," 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n"," 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 1. 1. 1.]\n","[[0.01348772 0.98651228]\n"," [0.99687678 0.00312322]\n"," [0.46799142 0.53200858]\n"," ...\n"," [0.00367441 0.99632559]\n"," [0.00267083 0.99732917]\n"," [0.17049783 0.82950217]]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjY3nV6XPndk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670448325065,"user_tz":0,"elapsed":5137,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"5066bdfd-6544-4f95-cec2-509f05f57da1"},"outputs":[{"output_type":"stream","name":"stdout","text":["  + Number of params: 2516592\n"]}],"source":["########################################### FCN ################################################\n","########################################### FCN ################################################\n","########################################### FCN ################################################\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--ngpu', type=int, default=1)\n","parser.add_argument('--seed', type=int, default=1)\n","parser.add_argument('--no-cuda', action='store_true')\n","parser.add_argument('--input_c', type=int, default=new_number_features)\n","parser.add_argument('--hid_1', type=int, default=500)\n","parser.add_argument('--hid_2', type=int, default=30)\n","parser.add_argument('--out_c', type=int, default=2)\n","parser.add_argument('--dropout_rate', type=int, default=0.5)\n","\n","args, unknown = parser.parse_known_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","model_1 = FCN(input_c=args.input_c, hid_1=args.hid_1, hid_2=args.hid_2, out_c=args.out_c, dropout=args.dropout_rate)\n","model_2 = FCN(input_c=args.input_c, hid_1=args.hid_1, hid_2=args.hid_2, out_c=args.out_c, dropout=args.dropout_rate)\n","model_3 = FCN(input_c=args.input_c, hid_1=args.hid_1, hid_2=args.hid_2, out_c=args.out_c, dropout=args.dropout_rate)\n","model_4 = FCN(input_c=args.input_c, hid_1=args.hid_1, hid_2=args.hid_2, out_c=args.out_c, dropout=args.dropout_rate)\n","model_5 = FCN(input_c=args.input_c, hid_1=args.hid_1, hid_2=args.hid_2, out_c=args.out_c, dropout=args.dropout_rate)\n","\n","gpu_ids = range(args.ngpu)\n","model_1 = nn.parallel.DataParallel(model_1, device_ids=gpu_ids)\n","model_2 = nn.parallel.DataParallel(model_2, device_ids=gpu_ids)\n","model_3 = nn.parallel.DataParallel(model_3, device_ids=gpu_ids)\n","model_4 = nn.parallel.DataParallel(model_4, device_ids=gpu_ids)\n","model_5 = nn.parallel.DataParallel(model_5, device_ids=gpu_ids)\n","\n","print('  + Number of params: {}'.format(\n","    sum([p.data.nelement() for p in model_1.parameters()])))\n","if args.cuda:\n","    model_1 = model_1.cuda()\n","    model_2 = model_2.cuda()\n","    model_3 = model_3.cuda()\n","    model_4 = model_4.cuda()\n","    model_5 = model_5.cuda()\n","\n","\n","model_1 = load_model(model_1, save_path, 'FCN_' + str(1))\n","model_2 = load_model(model_2, save_path, 'FCN_' + str(2))\n","model_3 = load_model(model_3, save_path, 'FCN_' + str(3))\n","model_4 = load_model(model_4, save_path, 'FCN_' + str(4))\n","model_5 = load_model(model_5, save_path, 'FCN_' + str(5))"]},{"cell_type":"code","source":["for fold in range(1, k_fold+1):\n","    if fold == 1:\n","        model = model_1.eval()\n","    elif fold == 2:\n","        model = model_2.eval()\n","    elif fold == 3:\n","        model = model_3.eval()\n","    elif fold == 4:\n","        model = model_4.eval()\n","    elif fold == 5:\n","        model = model_5.eval()\n","    \n","    fold_test_index = dist_test[str(fold)]\n","\n","    model.eval()\n","\n","    data = torch.from_numpy(FCN_data[str(fold)][0])\n","    data = Variable(data, requires_grad=False)\n","    data = data.float()\n","\n","    target = torch.from_numpy(FCN_data[str(fold)][1])\n","    target = target.float().long()\n","\n","    if args.cuda:\n","        data, target = data.cuda(), target.cuda()\n","    \n","    data_test = data[fold_test_index,:] \n","    target = target[fold_test_index, :]\n","    out = model(data_test)\n","\n","    out_prob = torch.exp(out)\n","    out = torch.max(out, 1)[1]\n","    target = torch.max(target, 1)[1]\n","    target = target.cpu().numpy()\n","    out = out.cpu().numpy()\n","    out_prob = out_prob.cpu().detach().numpy()\n","    if fold == 1:\n","      FCN_labels_concat = target\n","      FCN_results_concat = out\n","      FCN_results_prob_concat = out_prob\n","    else:\n","      FCN_labels_concat = np.concatenate((FCN_labels_concat, target),axis = 0)\n","      FCN_results_concat = np.concatenate((FCN_results_concat, out),axis = 0)\n","      FCN_results_prob_concat = np.concatenate((FCN_results_prob_concat, out_prob),axis = 0)\n","\n","FCN_labels_concat = 1 - FCN_labels_concat\n","FCN_results_concat = 1 - FCN_results_concat\n","FCN_results_prob_concat_change = np.zeros_like(FCN_results_prob_concat)\n","FCN_results_prob_concat_change[:,0] = FCN_results_prob_concat[:,1]\n","FCN_results_prob_concat_change[:,1] = FCN_results_prob_concat[:,0]\n","\n","test_accuracy = metrics.accuracy_score(FCN_labels_concat, FCN_results_concat)\n","print(test_accuracy)\n","print(FCN_labels_concat)\n","print(FCN_results_concat)\n","print(FCN_results_prob_concat_change)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnrSW-Q-174B","executionInfo":{"status":"ok","timestamp":1670448325070,"user_tz":0,"elapsed":25,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"d37451df-6995-4a8b-b05e-f48aa03825cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6781609195402298\n","[1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n"," 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1\n"," 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n"," 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n"," 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n"," 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n"," 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n"," 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0\n"," 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n"," 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n","[1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n"," 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1\n"," 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0\n"," 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0\n"," 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n"," 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n"," 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n"," 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0\n"," 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n"," 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0\n"," 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1\n"," 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1\n"," 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n"," 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n"," 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1\n"," 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n"," 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0\n"," 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n"," 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n"," 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1\n"," 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1]\n","[[4.6943399e-04 9.9953055e-01]\n"," [9.9823916e-01 1.7608561e-03]\n"," [5.2547878e-01 4.7452125e-01]\n"," ...\n"," [2.8364190e-03 9.9716359e-01]\n"," [7.0029224e-04 9.9929965e-01]\n"," [1.0416997e-01 8.9583004e-01]]\n"]}]},{"cell_type":"code","source":["########################################### AUTO ################################################\n","########################################### AUTO ################################################\n","########################################### AUTO ################################################\n","torch.cuda.empty_cache()\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--ngpu', type=int, default=1)\n","parser.add_argument('--no-cuda', action='store_true')\n","parser.add_argument('--seed', type=int, default=1)\n","parser.add_argument('--auto_in', type=int, default=new_number_features)\n","parser.add_argument('--auto_hid_1', type=int, default=300)\n","parser.add_argument('--auto_hid_2', type=int, default=150)\n","parser.add_argument('--auto_hid_3', type=int, default=300)\n","parser.add_argument('--MLP_1', type=int, default=300)\n","parser.add_argument('--MLP_2', type=int, default=16)\n","parser.add_argument('--MLP_out', type=int, default=2)\n","parser.add_argument('--dropout_rate', type=int, default=0.5)\n","\n","args, unknown = parser.parse_known_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","    \n","print(\"build AUTO pytorch\")\n","model_1 = Auto_encoder_MLP(in_c=args.auto_in, auto_1=args.auto_hid_1, auto_2=args.auto_hid_2, auto_3=args.auto_hid_3, MLP_1=args.MLP_1, MLP_2=args.MLP_2, MLP_out=args.MLP_out, dropout_rate=args.dropout_rate)\n","model_2 = Auto_encoder_MLP(in_c=args.auto_in, auto_1=args.auto_hid_1, auto_2=args.auto_hid_2, auto_3=args.auto_hid_3, MLP_1=args.MLP_1, MLP_2=args.MLP_2, MLP_out=args.MLP_out, dropout_rate=args.dropout_rate)\n","model_3 = Auto_encoder_MLP(in_c=args.auto_in, auto_1=args.auto_hid_1, auto_2=args.auto_hid_2, auto_3=args.auto_hid_3, MLP_1=args.MLP_1, MLP_2=args.MLP_2, MLP_out=args.MLP_out, dropout_rate=args.dropout_rate)\n","model_4 = Auto_encoder_MLP(in_c=args.auto_in, auto_1=args.auto_hid_1, auto_2=args.auto_hid_2, auto_3=args.auto_hid_3, MLP_1=args.MLP_1, MLP_2=args.MLP_2, MLP_out=args.MLP_out, dropout_rate=args.dropout_rate)\n","model_5 = Auto_encoder_MLP(in_c=args.auto_in, auto_1=args.auto_hid_1, auto_2=args.auto_hid_2, auto_3=args.auto_hid_3, MLP_1=args.MLP_1, MLP_2=args.MLP_2, MLP_out=args.MLP_out, dropout_rate=args.dropout_rate)\n","\n","gpu_ids = range(args.ngpu)\n","model_1 = nn.parallel.DataParallel(model_1, device_ids=gpu_ids)\n","model_2 = nn.parallel.DataParallel(model_2, device_ids=gpu_ids)\n","model_3 = nn.parallel.DataParallel(model_3, device_ids=gpu_ids)\n","model_4 = nn.parallel.DataParallel(model_4, device_ids=gpu_ids)\n","model_5 = nn.parallel.DataParallel(model_5, device_ids=gpu_ids)\n","\n","print('  + Number of params: {}'.format(\n","    sum([p.data.nelement() for p in model_1.parameters()])))\n","if args.cuda:\n","    model_1 = model_1.cuda()\n","    model_2 = model_2.cuda()\n","    model_3 = model_3.cuda()\n","    model_4 = model_4.cuda()\n","    model_5 = model_5.cuda()\n","\n","model_1 = load_model(model_1, save_path, 'AUTO_' + str(1))\n","model_2 = load_model(model_2, save_path, 'AUTO_' + str(2))\n","model_3 = load_model(model_3, save_path, 'AUTO_' + str(3))\n","model_4 = load_model(model_4, save_path, 'AUTO_' + str(4))\n","model_5 = load_model(model_5, save_path, 'AUTO_' + str(5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kqp_V7wp86X3","executionInfo":{"status":"ok","timestamp":1670448328146,"user_tz":0,"elapsed":3099,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"38fc92b2-a8a8-4622-f97a-a31502c00cbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["build AUTO pytorch\n","  + Number of params: 4600900\n"]}]},{"cell_type":"code","source":["for fold in range(1, k_fold+1):\n","    if fold == 1:\n","        model = model_1.eval()\n","    elif fold == 2:\n","        model = model_2.eval()\n","    elif fold == 3:\n","        model = model_3.eval()\n","    elif fold == 4:\n","        model = model_4.eval()\n","    elif fold == 5:\n","        model = model_5.eval()\n","    \n","    fold_test_index = dist_test[str(fold)]\n","\n","    model.eval()\n","\n","    data = torch.from_numpy(FCN_data[str(fold)][0])\n","    data = Variable(data, requires_grad=False)\n","    data = data.float()\n","\n","    target = torch.from_numpy(FCN_data[str(fold)][1])\n","    target = target.float().long()\n","\n","    if args.cuda:\n","        data, target = data.cuda(), target.cuda()\n","    \n","    data_test = data[fold_test_index,:] \n","    target = target[fold_test_index, :]\n","    out,out_auto = model(data_test)\n","\n","    out_prob = torch.exp(out)\n","    out = torch.max(out, 1)[1]\n","    target = torch.max(target, 1)[1]\n","    target = target.cpu().numpy()\n","    out = out.cpu().numpy()\n","    out_prob = out_prob.cpu().detach().numpy()\n","\n","    if fold == 1:\n","      AUTO_labels_concat = target\n","      AUTO_results_concat = out\n","      AUTO_results_prob_concat = out_prob\n","    else:\n","      AUTO_labels_concat = np.concatenate((AUTO_labels_concat, target),axis = 0)\n","      AUTO_results_concat = np.concatenate((AUTO_results_concat, out),axis = 0)\n","      AUTO_results_prob_concat = np.concatenate((AUTO_results_prob_concat, out_prob),axis = 0)\n","\n","AUTO_labels_concat = 1 - AUTO_labels_concat\n","AUTO_results_concat = 1 - AUTO_results_concat\n","AUTO_results_prob_concat_change = np.zeros_like(AUTO_results_prob_concat)\n","AUTO_results_prob_concat_change[:,0] = AUTO_results_prob_concat[:,1]\n","AUTO_results_prob_concat_change[:,1] = AUTO_results_prob_concat[:,0]\n","\n","test_accuracy = metrics.accuracy_score(AUTO_labels_concat, AUTO_results_concat)\n","print(test_accuracy)\n","print(AUTO_labels_concat)\n","print(AUTO_results_concat)\n","print(AUTO_results_prob_concat_change)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgUS_DLo9d6A","executionInfo":{"status":"ok","timestamp":1670448328434,"user_tz":0,"elapsed":300,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"9afcb575-b14f-46eb-d8ee-1dd28643a4a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6908045977011494\n","[1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n"," 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1\n"," 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n"," 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n"," 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n"," 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n"," 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n"," 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0\n"," 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n"," 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n","[1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0\n"," 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0\n"," 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0\n"," 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0\n"," 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1\n"," 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n"," 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1\n"," 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0\n"," 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n"," 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0\n"," 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n"," 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1\n"," 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0\n"," 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0\n"," 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1\n"," 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1\n"," 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0\n"," 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0\n"," 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n"," 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0\n"," 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1\n"," 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1]\n","[[8.4777438e-04 9.9915218e-01]\n"," [9.9996865e-01 3.1373202e-05]\n"," [9.9776042e-01 2.2395134e-03]\n"," ...\n"," [8.9721046e-03 9.9102789e-01]\n"," [3.7774364e-05 9.9996221e-01]\n"," [7.8256316e-02 9.2174369e-01]]\n"]}]},{"cell_type":"code","source":["########################################### GCN ################################################\n","########################################### GCN ################################################\n","########################################### GCN ################################################\n","torch.cuda.empty_cache()\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--ngpu', type=int, default=1)\n","parser.add_argument('--no-cuda', action='store_true')\n","parser.add_argument('--seed', type=int, default=1)\n","parser.add_argument('--cheby_order_K', type=int, default=2)\n","parser.add_argument('--input_dimension', type=int, default=new_number_features)\n","parser.add_argument('--hidden_dimension', type=int, default=128)\n","parser.add_argument('--output_dimension', type=int, default=2)\n","parser.add_argument('--dropout_rate', type=int, default=0.3)\n","#\n","args, unknown = parser.parse_known_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","    \n","print(\"build GCN pytorch\")\n","model_1 = GCN(in_c=args.input_dimension, hid_c=args.hidden_dimension, out_c=args.output_dimension, K=args.cheby_order_K, dropout_rate=args.dropout_rate)\n","model_2 = GCN(in_c=args.input_dimension, hid_c=args.hidden_dimension, out_c=args.output_dimension, K=args.cheby_order_K, dropout_rate=args.dropout_rate)\n","model_3 = GCN(in_c=args.input_dimension, hid_c=args.hidden_dimension, out_c=args.output_dimension, K=args.cheby_order_K, dropout_rate=args.dropout_rate)\n","model_4 = GCN(in_c=args.input_dimension, hid_c=args.hidden_dimension, out_c=args.output_dimension, K=args.cheby_order_K, dropout_rate=args.dropout_rate)\n","model_5 = GCN(in_c=args.input_dimension, hid_c=args.hidden_dimension, out_c=args.output_dimension, K=args.cheby_order_K, dropout_rate=args.dropout_rate)\n","\n","gpu_ids = range(args.ngpu)\n","model_1 = nn.parallel.DataParallel(model_1, device_ids=gpu_ids)\n","model_2 = nn.parallel.DataParallel(model_2, device_ids=gpu_ids)\n","model_3 = nn.parallel.DataParallel(model_3, device_ids=gpu_ids)\n","model_4 = nn.parallel.DataParallel(model_4, device_ids=gpu_ids)\n","model_5 = nn.parallel.DataParallel(model_5, device_ids=gpu_ids)\n","\n","print('  + Number of params: {}'.format(\n","    sum([p.data.nelement() for p in model_1.parameters()])))\n","if args.cuda:\n","    model_1 = model_1.cuda()\n","    model_2 = model_2.cuda()\n","    model_3 = model_3.cuda()\n","    model_4 = model_4.cuda()\n","    model_5 = model_5.cuda()\n","\n","model_1 = load_model(model_1, save_path, 'GCN_' + str(1))\n","model_2 = load_model(model_2, save_path, 'GCN_' + str(2))\n","model_3 = load_model(model_3, save_path, 'GCN_' + str(3))\n","model_4 = load_model(model_4, save_path, 'GCN_' + str(4))\n","model_5 = load_model(model_5, save_path, 'GCN_' + str(5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2gpuBzB-X4Q","executionInfo":{"status":"ok","timestamp":1670448331040,"user_tz":0,"elapsed":2607,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"b0372ace-121c-45cc-c110-c7027f961280"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["build GCN pytorch\n","  + Number of params: 1313538\n"]}]},{"cell_type":"code","source":["for fold in range(1, k_fold+1):\n","    if fold == 1:\n","        model = model_1.eval()\n","    elif fold == 2:\n","        model = model_2.eval()\n","    elif fold == 3:\n","        model = model_3.eval()\n","    elif fold == 4:\n","        model = model_4.eval()\n","    elif fold == 5:\n","        model = model_5.eval()\n","    \n","    fold_test_index = dist_test[str(fold)]\n","\n","    model.eval()\n","\n","    data = torch.from_numpy(GCN_data[str(fold)][0])\n","    data = Variable(data, requires_grad=False)\n","    data = data.float()\n","\n","    target = torch.from_numpy(GCN_data[str(fold)][1])\n","    target = target.float().long()\n","\n","    edge_index = torch.from_numpy(GCN_data[str(fold)][2])\n","    edge_index = Variable(edge_index, requires_grad=False)\n","\n","    edgenet_input = torch.from_numpy(GCN_data[str(fold)][3])\n","    edgenet_input = Variable(edgenet_input, requires_grad=False)\n","    edgenet_input = edgenet_input.float()\n","\n","    if args.cuda:\n","        data, target, edge_index, edgenet_input = data.cuda(), target.cuda(), edge_index.cuda(), edgenet_input.cuda()\n","\n","    out = model(data, edge_index, edgenet_input)\n","    out = out[fold_test_index, :]\n","    target = target[fold_test_index, :]\n","    out_prob = torch.exp(out)\n","\n","    out = torch.max(out, 1)[1]\n","    target = torch.max(target, 1)[1]\n","    target = target.cpu().numpy()\n","    out = out.cpu().numpy()\n","    out_prob = out_prob.cpu().detach().numpy()\n","\n","    if fold == 1:\n","      GCN_labels_concat = target\n","      GCN_results_concat = out\n","      GCN_results_prob_concat = out_prob\n","    else:\n","      GCN_labels_concat = np.concatenate((GCN_labels_concat, target),axis = 0)\n","      GCN_results_concat = np.concatenate((GCN_results_concat, out),axis = 0)\n","      GCN_results_prob_concat = np.concatenate((GCN_results_prob_concat, out_prob),axis = 0)\n","\n","GCN_labels_concat = 1 - GCN_labels_concat\n","GCN_results_concat = 1 - GCN_results_concat\n","GCN_results_prob_concat_change = np.zeros_like(GCN_results_prob_concat)\n","GCN_results_prob_concat_change[:,0] = GCN_results_prob_concat[:,1]\n","GCN_results_prob_concat_change[:,1] = GCN_results_prob_concat[:,0]\n","\n","test_accuracy = metrics.accuracy_score(GCN_labels_concat, GCN_results_concat)\n","print(test_accuracy)\n","print(GCN_labels_concat)\n","print(GCN_results_concat)\n","print(GCN_results_prob_concat_change)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YL0LUx1IBMw_","executionInfo":{"status":"ok","timestamp":1670448354409,"user_tz":0,"elapsed":23371,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"dfbd74e2-f525-4b3e-e101-10c3c961dca6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6666666666666666\n","[1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n"," 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1\n"," 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n"," 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n"," 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n"," 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n"," 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n"," 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0\n"," 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n"," 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n","[1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0\n"," 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1\n"," 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0\n"," 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0\n"," 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1\n"," 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1\n"," 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n"," 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1\n"," 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n"," 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1\n"," 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0\n"," 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1\n"," 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1\n"," 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n"," 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n"," 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n"," 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n"," 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1\n"," 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0\n"," 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1\n"," 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0\n"," 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n"," 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1]\n","[[9.0697822e-06 9.9999094e-01]\n"," [9.9937350e-01 6.2651542e-04]\n"," [1.9108330e-01 8.0891675e-01]\n"," ...\n"," [7.2489441e-03 9.9275100e-01]\n"," [4.5526340e-03 9.9544734e-01]\n"," [1.4615688e-01 8.5384309e-01]]\n"]}]},{"cell_type":"code","source":["########################################### EV_GCN ################################################\n","########################################### EV_GCN ################################################\n","########################################### EV_GCN ################################################\n","torch.cuda.empty_cache()\n","\n","torch.cuda.empty_cache()\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--ngpu', type=int, default=1)\n","parser.add_argument('--no-cuda', action='store_true')\n","parser.add_argument('--seed', type=int, default=1)\n","parser.add_argument('--input_dimension', type=int, default=new_number_features)\n","parser.add_argument('--hgc', type=int, default=16, help='hidden units of gconv layer')\n","parser.add_argument('--lg', type=int, default=4, help='number of gconv layers')\n","parser.add_argument('--lr', default=0.01, type=float, help='initial learning rate')\n","parser.add_argument('--wd', default=5e-5, type=float, help='weight decay')\n","parser.add_argument('--num_iter', default=300, type=int, help='number of epochs for training')\n","parser.add_argument('--edropout', type=float, default=0.3, help='edge dropout rate')\n","parser.add_argument('--dropout', default=0.2, type=float, help='ratio of dropout')\n","parser.add_argument('--num_classes', type=int, default=2, help='number of classes')\n","\n","#\n","args, unknown = parser.parse_known_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","    \n","print(\"build EV_GCN pytorch\")\n","\n","model_1 = EV_GCN(args.input_dimension, args.num_classes, args.dropout, edge_dropout=args.edropout, hgc=args.hgc, lg=args.lg, edgenet_input_dim=2*phonetic_data.shape[1])\n","model_2 = EV_GCN(args.input_dimension, args.num_classes, args.dropout, edge_dropout=args.edropout, hgc=args.hgc, lg=args.lg, edgenet_input_dim=2*phonetic_data.shape[1])\n","model_3 = EV_GCN(args.input_dimension, args.num_classes, args.dropout, edge_dropout=args.edropout, hgc=args.hgc, lg=args.lg, edgenet_input_dim=2*phonetic_data.shape[1])\n","model_4 = EV_GCN(args.input_dimension, args.num_classes, args.dropout, edge_dropout=args.edropout, hgc=args.hgc, lg=args.lg, edgenet_input_dim=2*phonetic_data.shape[1])\n","model_5 = EV_GCN(args.input_dimension, args.num_classes, args.dropout, edge_dropout=args.edropout, hgc=args.hgc, lg=args.lg, edgenet_input_dim=2*phonetic_data.shape[1])\n","\n","gpu_ids = range(args.ngpu)\n","model_1 = nn.parallel.DataParallel(model_1, device_ids=gpu_ids)\n","model_2 = nn.parallel.DataParallel(model_2, device_ids=gpu_ids)\n","model_3 = nn.parallel.DataParallel(model_3, device_ids=gpu_ids)\n","model_4 = nn.parallel.DataParallel(model_4, device_ids=gpu_ids)\n","model_5 = nn.parallel.DataParallel(model_5, device_ids=gpu_ids)\n","\n","print('  + Number of params: {}'.format(\n","    sum([p.data.nelement() for p in model_1.parameters()])))\n","if args.cuda:\n","    model_1 = model_1.cuda()\n","    model_2 = model_2.cuda()\n","    model_3 = model_3.cuda()\n","    model_4 = model_4.cuda()\n","    model_5 = model_5.cuda()\n","\n","model_1 = load_model(model_1, save_path, 'EV_GCN_' + str(1))\n","model_2 = load_model(model_2, save_path, 'EV_GCN_' + str(2))\n","model_3 = load_model(model_3, save_path, 'EV_GCN_' + str(3))\n","model_4 = load_model(model_4, save_path, 'EV_GCN_' + str(4))\n","model_5 = load_model(model_5, save_path, 'EV_GCN_' + str(5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xs0YLZYAE7Qs","executionInfo":{"status":"ok","timestamp":1670448356908,"user_tz":0,"elapsed":2294,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"e5be09e0-ca7b-4d41-adc7-56b74a8da157"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["build EV_GCN pytorch\n","  + Number of params: 196482\n"]}]},{"cell_type":"code","source":["for fold in range(1, k_fold+1):\n","    if fold == 1:\n","        model = model_1.eval()\n","    elif fold == 2:\n","        model = model_2.eval()\n","    elif fold == 3:\n","        model = model_3.eval()\n","    elif fold == 4:\n","        model = model_4.eval()\n","    elif fold == 5:\n","        model = model_5.eval()\n","    \n","    fold_test_index = dist_test[str(fold)]\n","\n","    model.eval()\n","\n","    data = torch.from_numpy(EV_GCN_data[str(fold)][0])\n","    data = Variable(data, requires_grad=False)\n","    data = data.float()\n","\n","    target = torch.from_numpy(EV_GCN_data[str(fold)][1])\n","    target = target.float().long()\n","\n","    edge_index = torch.from_numpy(EV_GCN_data[str(fold)][2])\n","    edge_index = Variable(edge_index, requires_grad=False)\n","\n","    edgenet_input = torch.from_numpy(EV_GCN_data[str(fold)][3])\n","    edgenet_input = Variable(edgenet_input, requires_grad=False)\n","    edgenet_input = edgenet_input.float()\n","\n","    if args.cuda:\n","        data, target, edge_index, edgenet_input = data.cuda(), target.cuda(), edge_index.cuda(), edgenet_input.cuda()\n","\n","    out, _ = model(data, edge_index, edgenet_input)\n","    out = out[fold_test_index, :]\n","    target = target[fold_test_index, :]\n","\n","    out_prob = torch.exp(out)\n","    out = torch.max(out, 1)[1]\n","    target = torch.max(target, 1)[1]\n","    target = target.cpu().numpy()\n","    out = out.cpu().numpy()\n","    out_prob = out_prob.cpu().detach().numpy()\n","\n","    if fold == 1:\n","      EV_GCN_labels_concat = target\n","      EV_GCN_results_concat = out\n","      EV_GCN_results_prob_concat = out_prob\n","    else:\n","      EV_GCN_labels_concat = np.concatenate((EV_GCN_labels_concat, target),axis = 0)\n","      EV_GCN_results_concat = np.concatenate((EV_GCN_results_concat, out),axis = 0)\n","      EV_GCN_results_prob_concat = np.concatenate((EV_GCN_results_prob_concat, out_prob),axis = 0)\n","\n","EV_GCN_labels_concat = 1 - EV_GCN_labels_concat\n","EV_GCN_results_concat = 1 - EV_GCN_results_concat\n","EV_GCN_results_prob_concat_change = np.zeros_like(EV_GCN_results_prob_concat)\n","EV_GCN_results_prob_concat_change[:,0] = EV_GCN_results_prob_concat[:,1]\n","EV_GCN_results_prob_concat_change[:,1] = EV_GCN_results_prob_concat[:,0]\n","\n","test_accuracy = metrics.accuracy_score(EV_GCN_labels_concat, EV_GCN_results_concat)\n","print(test_accuracy)\n","print(EV_GCN_labels_concat)\n","print(EV_GCN_results_concat)\n","print(EV_GCN_results_prob_concat_change)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4u8n2Hy2HSfQ","executionInfo":{"status":"ok","timestamp":1670448371970,"user_tz":0,"elapsed":15067,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"6fad1935-1bfe-4140-8faa-eac0482ee5e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.664367816091954\n","[1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n"," 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n"," 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1\n"," 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1\n"," 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n"," 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n"," 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n"," 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n"," 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n"," 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n"," 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n"," 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0\n"," 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n"," 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n","[1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0\n"," 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1\n"," 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0\n"," 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n"," 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n"," 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n"," 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n"," 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1\n"," 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n"," 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1\n"," 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0\n"," 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1\n"," 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1\n"," 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0\n"," 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n"," 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1\n"," 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n"," 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n"," 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0\n"," 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1\n"," 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n"," 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n"," 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1]\n","[[4.5439712e-08 1.0000000e+00]\n"," [1.0000000e+00 2.0420812e-09]\n"," [9.9828160e-01 1.7183722e-03]\n"," ...\n"," [8.3840333e-02 9.1615969e-01]\n"," [1.2666759e-09 1.0000000e+00]\n"," [3.4729009e-03 9.9652714e-01]]\n"]}]},{"cell_type":"code","source":["########################################### EMMA ################################################\n","########################################### EMMA ################################################\n","########################################### EMMA ################################################\n","\n","EMMA_results = SVM_results_concat + FCN_results_concat + AUTO_results_concat + GCN_results_concat + EV_GCN_results_concat\n","EMMA_results_prob = (SVM_results_prob_concat + FCN_results_prob_concat + AUTO_results_prob_concat + GCN_results_prob_concat + EV_GCN_results_prob_concat)/5\n","EMMA_results[EMMA_results<2.5] = 0\n","EMMA_results[EMMA_results>=2.5] = 1\n","\n","test_accuracy = metrics.accuracy_score(test_labels_concat, EMMA_results)\n","test_fpr, test_tpr, te_thresholds = roc_curve(test_labels_concat, EMMA_results_prob[:,0],pos_label=1)\n","test_auc = auc(test_fpr, test_tpr)\n","print('EMMA accuracy is: ', test_accuracy)\n","print('EMMA AUC is: ', test_auc)\n","\n","\n","plt.grid()\n","plt.plot(test_fpr, test_tpr, label=\" AUC TEST =\"+str(auc(test_fpr, test_tpr)))\n","plt.plot([0,1],[0,1],'g--')\n","plt.legend()\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"Test AUC(ROC curve)\")\n","plt.grid(color='black', linestyle='-', linewidth=0.5)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"L2fiO3eQH3Cl","executionInfo":{"status":"ok","timestamp":1670448371971,"user_tz":0,"elapsed":35,"user":{"displayName":"董一澜","userId":"04082425908344234200"}},"outputId":"a6be1c31-629b-4a92-d57a-d18de9a7b3f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EMMA accuracy is:  0.6793103448275862\n","EMMA AUC is:  0.7286783810925552\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMipH34CU/qOnmJhx9WxayS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}